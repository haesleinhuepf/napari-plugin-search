{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adebf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib, json\n",
    "\n",
    "def get_plugin_index():\n",
    "    url = 'https://api.napari-hub.org/plugins'\n",
    "\n",
    "    json_url = urllib.request.urlopen(url) \n",
    "    data = json.loads(json_url.read()) \n",
    "    #print (data)\n",
    "    return list(data.keys())\n",
    "\n",
    "# print(get_plugin_index())\n",
    "\n",
    "def get_plugin_details(plugin_name):\n",
    "    url = 'https://api.napari-hub.org/plugins/' + plugin_name\n",
    "\n",
    "    json_url = urllib.request.urlopen(url) \n",
    "    data = json.loads(json_url.read()) \n",
    "    #print (data)\n",
    "    return data\n",
    "\n",
    "def cache_descriptions():\n",
    "    all_plugins = get_plugin_index()\n",
    "    \n",
    "    all_descriptions = {}\n",
    "    for plugin in all_plugins:\n",
    "        all_descriptions[plugin] = get_plugin_details(plugin)['description']\n",
    "    return all_descriptions\n",
    "\n",
    "descriptions = cache_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8683707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['authors', 'citations', 'code_repository', 'description', 'description_content_type', 'description_text', 'development_status', 'documentation', 'first_released', 'license', 'name', 'operating_system', 'project_site', 'python_version', 'release_date', 'report_issues', 'requirements', 'summary', 'support', 'twitter', 'version'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = get_plugin_details('napari-pyclesperanto-assistant')\n",
    "details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "668b4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PartSeg': '# PartSeg\\n\\n![Tests](https://github.com/4DNucleome/PartSeg/workflows/Tests/badge.svg?branch=master)\\n[![PyPI version](https://badge.fury.io/py/PartSeg.svg)](https://badge.fury.io/py/PartSeg)\\n[![Documentation Status](https://readthedocs.org/projects/partseg/badge/?version=latest)](https://partseg.readthedocs.io/en/latest/?badge=latest)\\n[![Azure Pipelines Build Status](https://dev.azure.com/PartSeg/PartSeg/_apis/build/status/4DNucleome.PartSeg?branchName=master)](https://dev.azure.com/PartSeg/PartSeg/_build/latest?definitionId=1&branchName=master)\\n[![DOI](https://zenodo.org/badge/166421141.svg)](https://zenodo.org/badge/latestdoi/166421141)\\n[![Total alerts](https://img.shields.io/lgtm/alerts/g/4DNucleome/PartSeg.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/4DNucleome/PartSeg/alerts/)\\n[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/4DNucleome/PartSeg.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/4DNucleome/PartSeg/context:python)\\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/f9b0f1eb2c92486d9efd99ed5b2ef326)](https://www.codacy.com/gh/4DNucleome/PartSeg/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=4DNucleome/PartSeg&amp;utm_campaign=Badge_Grade)\\n[![Requirements Status](https://requires.io/github/4DNucleome/PartSeg/requirements.svg?branch=develop)](https://requires.io/github/4DNucleome/PartSeg/requirements/?branch=master)\\n[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\\n[![codecov](https://codecov.io/gh/4DNucleome/PartSeg/branch/master/graph/badge.svg?token=nbAbkOAe1C)](https://codecov.io/gh/4DNucleome/PartSeg)\\n\\nPartSeg is a GUI and a library for segmentation algorithms.\\n\\nThis application is designed to help biologist with segmentation based on threshold and connected components.\\n\\n![interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_analysis.png)\\n\\n## Tutorials\\n\\n-   Tutorial: **Chromosome 1 (as gui)** [link](https://github.com/4DNucleome/PartSeg/blob/master/tutorials/tutorial-chromosome-1/tutorial-chromosome1_16.md)\\n-   Data for chromosome 1 tutorial [link](https://4dnucleome.cent.uw.edu.pl/PartSeg/Downloads/PartSeg_samples.zip)\\n-   Tutorial: **Different neuron types (as library)** [link](https://github.com/4DNucleome/PartSeg/blob/master/tutorials/tutorial_neuron_types/Neuron_types_example.ipynb)\\n\\n## Installing\\n\\n-   From binaries:\\n    -   [Windows](https://4dnucleome.cent.uw.edu.pl/PartSeg/Downloads/PartSeg-lastest-windows.zip) (build on Windows 10)\\n    -   [Linux](https://4dnucleome.cent.uw.edu.pl/PartSeg/Downloads/PartSeg-lastest-linux.zip) (build on Ubuntu 18.04)\\n    -   [MacOS](https://4dnucleome.cent.uw.edu.pl/PartSeg/Downloads/PartSeg-lastest-macos.zip) (build on MacOS Mojave)\\n\\n-   With pip:\\n    -   From pypi: `pip install PartSeg[pyqt]`\\n    -   From repository: `pip install git+https://github.com/4DNucleome/PartSeg.git`\\n\\n## Running\\n\\nIf you downloaded binaries, run the `PartSeg` (or `PartSeg.exe` for Windows) file inside the `PartSeg` folder\\n\\nIf you installed from repository or from pip, you can run it with `PartSeg` command or `python -m PartSeg`.\\nFirst option does not work on Windows.\\n\\nPartSeg export few commandline options:\\n\\n-   `--no_report` - disable error reporting\\n-   `--no_dialog` - disable error reporting and error dialog. Use only when running from terminal.\\n-   `segmentation_analysis` - skip launcher and start analysis gui\\n-   `segmentation` - skip launcher and start segmentation gui\\n\\n## napari plugin\\n\\nPartSeg provides napari plugins for io to allow reading projects format in napari viewer.\\n\\n## Save Format\\n\\nSaved projects are tar files compressed with gzip or bz2.\\n\\nMetadata is saved in data.json file (in json format).\\nImages/masks are saved as *.npy (numpy array format).\\n\\n## Interface\\n\\nLauncher. Choose the program that you will launch:\\n\\n![launcher](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/launcher.png)\\n\\nMain window of Segmentation Analysis:\\n\\n![interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_analysis.png)\\n\\nMain window of Segmentation Analysis with view on measurement result:\\n\\n![interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_analysis2.png)\\n\\nWindow for creating a set of measurements:\\n\\n![statistics](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/measurement.png)\\n\\nMain window of Mask Segmentation:\\n\\n![mask interface](https://raw.githubusercontent.com/4DNucleome/PartSeg/master/images/roi_mask.png)\\n\\n## Laboratory\\n\\nLaboratory of Functional and Structural Genomics\\n[http://4dnucleome.cent.uw.edu.pl/](http://4dnucleome.cent.uw.edu.pl/)\\n\\n## Cite as\\n\\nBokota, G., Sroka, J., Basu, S. et al. PartSeg: a tool for quantitative feature extraction\\nfrom 3D microscopy images for dummies. BMC Bioinformatics 22, 72 (2021).\\n[https://doi.org/10.1186/s12859-021-03984-1](https://doi.org/10.1186/s12859-021-03984-1)\\n\\n\\n## Changelog\\n\\n### 0.13.9\\n\\n-   annotation show bugfix\\n\\n### 0.13.8\\n\\n-   napari deprecation fixes\\n-   speedup simple measurement\\n-   bundle plugins initial support\\n\\n### 0.13.7\\n\\n-   add measurements widget for napari\\n-   fix bug in pipeline usage\\n\\n### 0.13.6\\n\\n-   Hotfix release\\n-   Prepare for a new napari version\\n\\n### 0.13.5\\n\\n-   Small fixes for error reporting\\n-   Fix mask segmentation\\n\\n### 0.13.4\\n\\n-   Bugfix for outdated profile/pipeline preview\\n\\n### 0.13.3\\n\\n-   Fix saving roi_info in multiple files and history\\n\\n### 0.13.2\\n\\n-   Fix showing label in select label tab\\n\\n### 0.13.1\\n\\n-   Add Haralick measurements\\n-   Add obsep file support\\n\\n### 0.13.0\\n\\n-   Add possibility of custom input widgets for algorithms\\n-   Switch to napari Colormaps instead of custom one\\n-   Add points visualization\\n-   Synchronization widget for builtin (View menu) napari viewer\\n-   Drop Python 3.6\\n\\n### 0.12.7\\n\\n-   Fixes for napari 0.4.6\\n\\n### 0.12.6\\n\\n-   Fix prev_mask_get\\n-   Fix cache mechanism on mask change\\n-   Update PyInstaller build\\n\\n### 0.12.5\\n\\n-   Fix bug in pipeline execute\\n\\n### 0.12.4\\n\\n-   Fix ROI Mask windows related build (signal not properly connected)\\n\\n### 0.12.3\\n\\n-   Fix ROI Mask\\n\\n### 0.12.2\\n\\n-   Fix windows bundle\\n\\n### 0.12.1\\n\\n-   History of last opened files\\n-   Add ROI annotation and ROI alternatives\\n-   Minor bugfix\\n\\n### 0.12.0\\n\\n-   Toggle multiple files widget in View menu\\n-   Toggle Left panel in ROI Analysis in View Menu\\n-   Rename Mask Segmentation to ROI Mask\\n-   Add documentation for interface\\n-   Add Batch processing tutorial\\n-   Add information about errors to batch processing output file\\n-   Load image from the batch prepare window\\n-   Add search option in part of list and combo boxes\\n-   Add drag and drop mechanism to load list of files to batch window.\\n\\n### 0.11.5\\n\\n-   add side view to viewer\\n-   fix horizontal view for Measurements result table\\n\\n### 0.11.4\\n\\n-   bump to napari 0.3.8 in bundle\\n-   fix bug with not presented segmentation loaded from project\\n-   add frame (1 pix) to image cat from base one based on segmentation\\n-   pin to Qt version to 5.14\\n\\n### 0.11.3\\n\\n-   prepare for napari 0.3.7\\n-   split napari io plugin on multiple part\\n-   better reporting for numpy array via sentry\\n-   fix setting color for mask marking\\n\\n### 0.11.2\\n\\n-   Speedup image set in viewer using async calls\\n-   Fix bug in long name of sheet with parameters\\n\\n### 0.11.1\\n\\n-   Add screenshot option in View menu\\n-   Add Voxels measurements\\n\\n### 0.11.0\\n\\n-   Make sprawl algorithm name shorter\\n-   Unify capitalisation of measurement names\\n-   Add simple measurements to mask segmentation\\n-   Use napari as viewer\\n-   Add possibility to preview additional output of algorithms (In View menu)\\n-   Update names of available Algorithm and Measurement to be more descriptive.\\n\\n### 0.10.8\\n\\n-   fix synchronisation between viewers in Segmentation Analysis\\n-   fix batch crash on error during batch run, add information about file on which calculation fails\\n-   add changelog preview in Help > About\\n\\n### 0.10.7\\n\\n-   in measurements, on empty list of components mean will return 0\\n\\n### 0.10.6\\n\\n-   fix border rim preview\\n-   fix problem with size of image preview\\n-   zoom with scroll and moving if rectangle zoom is not marked\\n\\n### 0.10.5\\n\\n-   make PartSeg PEP517 compatible.\\n-   fix multiple files widget on Windows (path normalisation)\\n\\n### 0.10.4\\n\\n-   fix slow zoom\\n\\n### 0.10.3\\n\\n-   deterministic order of elements in batch processing.\\n\\n### 0.10.2\\n\\n-   bugfixes\\n\\n### 0.10.1\\n\\n-   bugfixes\\n\\n### 0.10.0\\n\\n-   Add creating custom label coloring.\\n-   Change execs interpreter to python 3.7.\\n-   Add masking operation in Segmentation Mask.\\n-   Change license to BSD.\\n-   Allow select root type in batch processing.\\n-   Add median filter in preview.\\n\\n### 0.9.7\\n\\n-   fix bug in compare mask\\n\\n### 0.9.6\\n\\n-   fix bug in loading project with mask\\n-   upgrade PyInstaller version (bug  GHSA-7fcj-pq9j-wh2r)\\n\\n### 0.9.5\\n\\n-   fix bug in loading project in \"Segmentation analysis\"\\n\\n### 0.9.4\\n\\n-   read mask segmentation projects\\n-   choose source type in batch\\n-   add initial support to OIF and CZI file format\\n-   extract utils to PartSegCore module\\n-   add automated tests of example notebook\\n-   reversed mask\\n-   load segmentation parameters in mask segmentation\\n-   allow use sprawl in segmentation tool\\n-   add radial split of mask for measurement\\n-   add all measurement results in batch, per component sheet\\n\\n### 0.9.3\\n\\n-   start automated build documentation\\n-   change color map backend and allow for user to create custom color map.\\n-   segmentation compare\\n-   update test engines\\n-   support of PySide2\\n\\n### 0.9.2.3\\n\\n-   refactor code to make easier create plugin for mask segmentation\\n-   create class base updater for update outdated algorithm description\\n-   fix save functions\\n-   fix different bugs\\n\\n### 0.9.2.2\\n\\n-   extract static data to separated package\\n-   update marker of fix range and add mark of gauss in channel control\\n\\n### 0.9.2.1\\n\\n-   add VoteSmooth and add choosing of smooth algorithm\\n\\n### 0.9.2\\n\\n-   add pypi base check for update\\n\\n-   remove resetting image state when change state in same image\\n\\n-   in stack segmentation add options to picking components from segmentation\\'s\\n\\n-   in mask segmentation add:\\n\\n    -   preview of segmentation parameters per component,\\n    -   save segmentation parameters in save file\\n    -   new implementation of batch mode.\\n\\n### 0.9.1\\n\\n-   Add multiple files widget\\n\\n-   Add Calculating distances between segmented object and mask\\n\\n-   Batch processing plan fixes:\\n\\n    -   Fix adding pipelines to plan\\n    -   Redesign mask widget\\n\\n-   modify measurement backend to allow calculate multi channel measurements.\\n\\n### 0.9\\n\\nBegin of changelog\\n\\n\\n',\n",
       " 'PlatyMatch': '[![DOI:10.1007/978-3-030-66415-2_30](https://zenodo.org/badge/DOI/10.1007/978-3-030-66415-2_30.svg)](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30)\\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\\n[![PyPI](https://img.shields.io/pypi/v/PlatyMatch.svg?color=green)](https://pypi.org/project/PlatyMatch)\\n[![Python Version](https://img.shields.io/pypi/pyversions/PlatyMatch.svg?color=green)](https://python.org)\\n[![tests](https://github.com/juglab/PlatyMatch/workflows/tests/badge.svg)](https://github.com/juglab/PlatyMatch/actions)\\n[![codecov](https://codecov.io/gh/juglab/PlatyMatch/branch/master/graph/badge.svg)](https://codecov.io/gh/juglab/PlatyMatch)\\n\\n\\n<p align=\"center\">\\n  <img src=\"https://user-images.githubusercontent.com/34229641/117537510-b26ee500-b001-11eb-9642-3baa461bfc94.png\" width=400 />\\n</p>\\n<h2 align=\"center\">Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence</h2>\\n\\n## Table of Contents\\n\\n- **[Introduction](#introduction)**\\n- **[Dependencies](#dependencies)**\\n- **[Getting Started](#getting-started)**\\n- **[Datasets](#datasets)**\\n- **[Registering your data](#registering-your-data)**\\n- **[Contributing](#contributing)**\\n- **[Issues](#issues)**\\n- **[Citation](#citation)**\\n\\n### Introduction\\nThis repository hosts the version of the code used for the **[publication](https://link.springer.com/chapter/10.1007/978-3-030-66415-2_30)** **Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence**. \\n\\nWe refer to the techniques elaborated in the publication, here as **PlatyMatch**. `PlatyMatch` performs a linear registration of volumetric, microscopy images of embryos by establishing correspondences between cells. \\n\\n`PlatyMatch` first detects nuclei in the two images being considered, next calculates unique `shape context` features for each nucleus detection which encapsulates the neighborhood as seen by that nucleus, and finally identifies pairs of matching nuclei through maximum bipartite matching applied to the pairwise distance matrix generated from these features. \\n\\n### Dependencies \\n\\nYou can install `PlatyMatch` via **[pip]**:\\n\\n```\\nconda create -y -n PlatyMatchEnv python==3.8\\nconda activate PlatyMatchEnv\\npython3 -m pip install PlatyMatch\\n```\\n\\n### Getting Started\\n\\nType in the following commands in a new terminal window.\\n\\n```\\nconda activate PlatyMatchEnv\\nnapari\\n```\\n\\nNext, select `PlatyMatch` from `Plugins> Add Dock Widget`.\\n\\n### Datasets\\n\\nDatasets are available in **`bic_eccv_data.zip`** as release assets **[here](https://github.com/juglab/PlatyMatch/releases/tag/v0.0.1)**.\\nThese comprise of images, nuclei detections and keypoint locations for confocal images of 12 individual specimens under the `01-insitus` directory and static snapshots of a live embryo imaged through Light Sheet Microscopy under the `02-live` directory. \\nFolders with the same name in these two directories correspond in their developmental age, for example, `01-insitus/02` corresponds to `02-live/02`, `01-insitus/03` corresponds to `02-live/03` and so on.   \\n\\n\\n### Registering your data\\n\\n- **Detect Nuclei** \\n\\t- Drag and drop your images in the viewer \\n\\t- Click on `Sync with Viewer` button to refresh the drop-down menus \\n\\t- Select the appropriate image in the drop down menu (for which nuclei detections are desired)\\n\\t- Select **`Detect Nuclei`** from the drop-down menu\\n\\t- Specify the anisotropy factor (`Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\\n\\t- Ideally min scales and max scales should be estimated from your data (`min_scale` should be set as `min_radius/sqrt(3)` and `max_scale` should be set as `max_radius/sqrt(3)`. The default values of `min_scale=5` and `max_scale=9` generally works well).  \\n\\t- Click `Run Scale Space Log` button. Please note that this step takes a few minutes.\\n\\t- Wait until a confirmation message suggesting that nuclei detection is over shows up on the terminal\\n\\t- Export the nuclei locations (`Export detections to csv`) to a csv file\\n\\t- Repeat this step for all images which need to be matched\\n\\n\\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120660618-cd5d3980-c487-11eb-8996-326264a4df87.mp4\\n\\n\\n- **Estimate Transform**\\n\\t- In case, nuclei were exported to a csv in the `Detect Nuclei` panel, tick `csv` checkbox\\n\\t- If the nuclei detected were specified in the order id, z, y and x in the csv file, then tick `IZYXR` checkbox\\n\\t- Additionally if there is a header in the csv file, tick `Header` checkbox\\n\\t- Load the detections for the `Moving Image`, which is defined as the image which will be transformed to later match another `fixed` image\\n\\t- Load the detections for the `Fixed Image`\\n\\t- Click on `Run` pushbutton. Once the calculation is complete, a confirmation message shows up in the terminal. Export the transform matrix to a csv (Note that this step can take a few minutes)\\n\\t- It is also possible to estimate the transform in a `supervised` fashion. For this, upload the locations of a few matching keypoints in both images. These locations serve to provide a good starting point for the transform calculation. Once the keypoint files have been uploaded for both the images, then click `Run` and then export the transform matrix to a csv file \\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120685628-53857a00-c4a0-11eb-8f92-7ffac730e28a.mp4\\n\\n\\n\\n- **Evaluate Metrics**\\n\\t- Drag images which need to be transformed, in the viewer\\n\\t- Click on `Sync with Viewer` button to refresh the drop-down menus\\n\\t- Specify the anisotropy factor (`Moving Image Anisotropy (Z)` and `Fixed Image Anisotropy (Z)`) (i.e. the ratio of the size of the z pixel with respect to the x or y pixel. This factor is typically more than 1.0 because the z dimension is often undersampled)\\n\\t- Load the transform which was calculated in the previous steps\\n\\t- If you simply wish to export a transformed version of the moving image, click on `Export Transformed Image`\\n\\t- Additionally, one could quantify metrics such as average registration error evaluated on a few keypoints. To do so, tick the `csv` checkbox, if keypoints and detections are available as a csv file. Then load the keypoints for the moving image (`Moving Kepoints`) and the fixed image (`Fixed Keypoints`)\\n\\t- Also, upload the detections calculated in the previous steps (`Detect Nuclei`)  by uploading the `Moving Detections` and the `Fixed Detections`\\n\\t- Click on the `Run` push button\\n\\t- The text fields such as `Matching Accuracy`(0 to 1, with 1 being the best) and `Average Registration Error` (the lower the better) should become populated once the results are available\\n\\n\\n\\nhttps://user-images.githubusercontent.com/34229641/120685654-5b451e80-c4a0-11eb-8d7d-de58b8b8304d.mp4\\n\\n\\n### Contributing\\n\\nContributions are very welcome. Tests can be run with **[tox]**.\\n\\n### Issues\\n\\nIf you encounter any problems, please **[file an issue]** along with a detailed description.\\n\\n[file an issue]: https://github.com/juglab/PlatyMatch/issues\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/EmbedSeg/\\n\\n\\n### Citation\\nIf you find our work useful in your research, please consider citing:\\n\\n```bibtex\\n@InProceedings{10.1007/978-3-030-66415-2_30,\\nauthor=\"Lalit, Manan and Handberg-Thorsager, Mette and Hsieh, Yu-Wen and Jug, Florian and Tomancak, Pavel\",\\neditor=\"Bartoli, Adrien\\nand Fusiello, Andrea\",\\ntitle=\"Registration of Multi-modal Volumetric Images by Establishing Cell Correspondence\",\\nbooktitle=\"Computer Vision -- ECCV 2020 Workshops\",\\nyear=\"2020\",\\npublisher=\"Springer International Publishing\",\\naddress=\"Cham\",\\npages=\"458--473\",\\nisbn=\"978-3-030-66415-2\"\\n}\\n```\\n\\n`PlatyMatch` plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/juglab/PlatyMatch/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'affinder': '# Description\\n\\nThis GUI plugin allows you to quickly find the affine matrix mapping\\none image to another using manual correspondence points annotation.\\n\\nMore simply, this plugin allows you to select corresponding points\\non an image, and a second image you wish to transform. It computes \\nthe requisite transformation matrix using Affine Transform, Euclidean Transform, \\nor Similarity Transform, and performs this transformation on the\\nmoving image, aligning it to the reference image.\\n\\nhttps://user-images.githubusercontent.com/17995243/120086403-f1d0b300-c121-11eb-8000-a44a2ac54339.mp4\\n\\n\\n# Who is This For?\\n\\nThis is a simple plugin which can be used on any 2D images, provided\\nthey can be loaded as layers into napari. The images need not be the same\\nfile format and this plugin also works with labels layers.\\n\\nNo prior understanding of the transformation methods is required, as\\nthey perform in the background based on the reference points selected.\\n\\n# How to Guide\\n\\nYou will need a combination of two or more 2D image and/or labels layers \\nloaded into napari. Once you have installed affinder, you can find it in\\nthe dock widgets menu.\\n\\n![Affinder widget in the Plugins->Add Dock Widget menu](https://i.imgur.com/w7MCXQy.png)\\n\\nThe first two dropdown boxes will be populated with the layers currently\\nloaded into napari. Select a layer to use as reference, and another to\\ntransform.\\n\\n![Dropdowns allow you to select the reference and moving layers](https://i.imgur.com/Tdbm1sX.png)\\n\\nNext, you can select the transformation model to use (affine is selected by default\\nand is the least rigid transformation of those available). See [below](#transformation-models) for a\\ndescription of the different models.\\n\\nFinally, you can optionally select a path to a text file for saving out the\\nresulting transformation matrix.\\n\\nWhen you click Start, affinder will add two points layers to napari. \\nThe plugin will also bring your reference image in focus, and its associated points\\nlayer. You can then start adding reference points by clicking on your image.\\n\\n![Adding reference points to layer](https://i.imgur.com/WPzNtyy.png)\\n\\nOnce three points are added, affinder will switch focus to the moving image,\\nand you should then proceed to select the corresponding three points.\\n\\n![Adding corresponding points to newly focused layer](https://i.imgur.com/JVZCvmp.png)\\n\\naffinder will immediately transform the moving image to align the points you\\'ve\\nselected when you add your third corresponding point to your moving image.\\n\\n![The moving image is transformed once three points are added](https://i.imgur.com/NTne9fj.png)\\n\\nFrom there, you can continue iteratively adding points until you \\nare happy with the alignment. Affinder will switch focus between\\nreference and moving image with each point.\\n\\nClick Finish to exit affinder.\\n\\n## Transformation Models\\n\\nThere are three transformation models available for use with affinder.\\nThey are listed here in order of increasing rigidity in the types of\\ntransforms they will allow. The eponymous Affine Transform is the \\nleast rigid and is the default choice.\\n\\n- [**Affine Transform**](https://en.wikipedia.org/wiki/Affine_transformation): \\nthe least rigid transformation, it preserves\\nlines and parallelism, but not necessarily distance and angles. Translation,\\nscaling, similarity, reflection, rotation and shearing are all valid\\naffine transformations.\\n\\n- [**Similarity Transform**](https://en.wikipedia.org/wiki/Similarity_(geometry)): \\nthis is a \"shape preserving\" transformation, producing objects which are \\ngeometrically similar. Translation, rotation, reflection and uniform scaling are \\nvalid similarity transforms. Shearing is not.\\n\\n- [**Euclidean Transform**](https://en.wikipedia.org/wiki/Rigid_transformation):\\nAlso known as a rigid transformation, this transform preserves the Euclidean\\ndistance between each pair of points on the image. This includes rotation,\\ntranslation and reflection but not scaling or shearing.\\n\\n# Getting Help\\n\\nIf you find a bug with affinder, or would like support with using it, please raise an\\nissue on the [GitHub repository](https://github.com/jni/affinder).\\n\\n# How to Cite\\n\\nMany plugins may be used in the course of published (or publishable) research, as well as\\nduring conference talks and other public facing events. If you\\'d like to be cited in\\na particular format, or have a DOI you\\'d like used, you should provide that information here.\\n',\n",
       " 'bfio': '# **B**io**F**ormats **I**nput/**O**utput utility (bfio v2.1.9)\\n\\n[![Documentation Status](https://readthedocs.org/projects/bfio/badge/?version=latest)](https://bfio.readthedocs.io/en/latest/?badge=latest)\\n[![PyPI](https://img.shields.io/pypi/v/bfio)](https://pypi.org/project/filepattern/)\\n![PyPI - Downloads](https://img.shields.io/pypi/dm/bfio)\\n![Bower](https://img.shields.io/bower/l/MI)\\n\\nThis tool is a simplified but powerful interface to \\n[Bioformats](https://www.openmicroscopy.org/bio-formats/)\\nusing jpype for direct access to the library. This tool is designed with\\nscalable image analysis in mind, with a simple interface to treat any image\\nlike a memory mapped array.\\n\\nDocker containers with all necessary components are available (see\\n**Docker Containers** section).\\n\\n## Summary\\n\\n  - [Installation](#installation)\\n  - [Docker](#docker)\\n  - [Documentation](#documentation)\\n  - [Contributing](#contributing)\\n  - [Versioning](#versioning)\\n  - [Authors](#authors)\\n  - [License](#license)\\n  - [Acknowledgments](#acknowledgments)\\n\\n## Installation\\n\\n### Setting up Java\\n\\n**Note:** `bfio` can be used without Java, but only the `python` and `zarr`\\nbackends will be useable. Only files in tiled OME Tiff or OME Zarr format can be\\nread/written.\\n\\nIn order to use the `Java` backend, it is necessary to first install the JDK.\\nThe `bfio` package is generally tested with\\n[JDK 8](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html),\\nbut JDK 11 and later also appear to work.\\n\\n### Installing bfio\\n\\nThe `bfio` package and the core dependencies (numpy, tifffile, imagecodecs) can\\nbe installed using pip:\\n\\n`pip install bfio`\\n\\nAdditionally, `bfio` with other dependencies can be installed:\\n\\n1. `pip install bfio[jpype]` - Adds support for BioFormats/Java\\n2. `pip install bfio[zarr]` - Adds support for OME Zarr\\n3. `pip install bfio[all]` - Installs all dependencies.\\n\\n## Docker\\n\\n### labshare/polus-bfio-util:2.1.8\\n\\nUbuntu based container with bfio and all dependencies (including Java).\\n\\n### labshare/polus-bfio-util:2.1.8-imagej\\n\\nSame as above, except comes with ImageJ and PyImageJ.\\n\\n### labshare/polus-bfio-util:2.1.8-tensorflow\\n\\nTensorflow container with bfio isntalled.\\n\\n## Documentation\\n\\nDocumentation and examples are available on\\n[Read the Docs](https://bfio.readthedocs.io/en/latest/).\\n\\n## Versioning\\n\\nWe use [SemVer](http://semver.org/) for versioning. For the versions\\navailable, see the [tags on this\\nrepository](https://github.com/PurpleBooth/a-good-readme-template/tags).\\n\\n## Authors\\n\\nNick Schaub (nick.schaub@nih.gov, nick.schaub@labshare.org)\\n\\n## License\\n\\nThis project is licensed under the [MIT License](LICENSE)\\nCreative Commons License - see the [LICENSE](LICENSE) file for\\ndetails\\n\\n## Acknowledgments\\n\\n  - Parts of this code were written/modified from existing code found in\\n    `tifffile`.\\n\\n\\n',\n",
       " 'brainglobe-napari-io': '# napari-cellfinder\\n\\n[![License](https://img.shields.io/pypi/l/brainglobe-napari-io.svg?color=green)](https://github.com/napari/brainglobe-napari-io/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/brainglobe-napari-io.svg?color=green)](https://pypi.org/project/brainglobe-napari-io)\\n[![Python Version](https://img.shields.io/pypi/pyversions/brainglobe-napari-io.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brainglobe/brainglobe-napari-io/workflows/tests/badge.svg)](https://github.com/brainglobe/brainglobe-napari-io/actions)\\n[![codecov](https://codecov.io/gh/brainglobe/brainglobe-napari-io/branch/master/graph/badge.svg)](https://codecov.io/gh/brainglobe/brainglobe-napari-io)\\n\\nVisualise cellfinder and brainreg results with napari\\n\\n\\n----------------------------------\\n\\n\\n## Installation\\nThis package is likely already installed \\n(e.g. with cellfinder, brainreg or another napari plugin), but if you want to \\ninstall it again, either use the napari plugin install GUI or you can \\ninstall `brainglobe-napari-io` via [pip]:\\n\\n    pip install brainglobe-napari-io\\n\\n## Usage\\n* Open napari (however you normally do it, but typically just type `napari` into your terminal, or click on your desktop icon)\\n\\n### brainreg\\n#### Sample space\\nDrag your [brainreg](https://github.com/brainglobe/brainreg) output directory (the one with the log file) onto the napari window.\\n\\nVarious images should then open, including:\\n* `Registered image` - the image used for registration, downsampled to atlas resolution\\n* `atlas_name` - e.g. `allen_mouse_25um` the atlas labels, warped to your sample brain\\n* `Boundaries` - the boundaries of the atlas regions\\n\\nIf you downsampled additional channels, these will also be loaded.\\n\\nMost of these images will not be visible by default. Click the little eye icon to toggle visibility.\\n\\n_N.B. If you use a high resolution atlas (such as `allen_mouse_10um`), then the files can take a little while to load._\\n\\n![sample_space](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/sample_space.gif)\\n\\n\\n#### Atlas space\\n`napari-brainreg` also comes with an additional plugin, for visualising your data \\nin atlas space. \\n\\nThis is typically only used in other software, but you can enable it yourself:\\n* Open napari\\n* Navigate to `Plugins` -> `Plugin Call Order`\\n* In the `Plugin Sorter` window, select `napari_get_reader` from the `select hook...` dropdown box\\n* Drag `brainreg_read_dir_standard_space` (the atlas space viewer plugin) above `brainreg_read_dir` (the normal plugin) to ensure that the atlas space plugin is used preferentially.\\n\\n\\n### cellfinder\\n#### Load cellfinder XML file\\n* Load your raw data (drag and drop the data directories into napari, one at a time)\\n* Drag and drop your cellfinder XML file (e.g. `cell_classification.xml`) into napari.\\n\\n#### Load cellfinder directory\\n* Load your raw data (drag and drop the data directories into napari, one at a time)\\n* Drag and drop your cellfinder output directory into napari.\\n\\nThe plugin will then load your detected cells (in yellow) and the rejected cell \\ncandidates (in blue). If you carried out registration, then these results will be \\noverlaid (similarly to the loading brainreg data, but transformed to the \\ncoordinate space of your raw data).\\n\\n![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_data.gif)\\n**Loading raw data**\\n\\n![load_data](https://raw.githubusercontent.com/brainglobe/brainglobe-napari-io/master/resources/load_results.gif)\\n**Loading cellfinder results**\\n\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"brainglobe-napari-io\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/brainglobe/brainglobe-napari-io/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n',\n",
       " 'brainreg-napari': \"[![Python Version](https://img.shields.io/pypi/pyversions/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari)\\n[![PyPI](https://img.shields.io/pypi/v/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari)\\n[![Wheel](https://img.shields.io/pypi/wheel/brainreg-napari.svg)](https://pypi.org/project/brainreg-napari)\\n[![Development Status](https://img.shields.io/pypi/status/brainreg-napari.svg)](https://github.com/brainglobe/brainreg-napari)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\\n\\n# brainreg-napari\\nNapari plugin to run [brainreg](https://github.com/brainglobe/brainreg), \\ndeveloped by [Stephen Lenzi](https://github.com/stephenlenzi).\\n\\n## Installation\\n```bash\\npip install brainreg-napari\\n```\\n\\n## Usage\\nDocumentation for the plugin is to come, but documentation for the original \\nbrainreg can be found [here](https://docs.brainglobe.info/brainreg/introduction) \\nand a tutorial is [here](https://docs.brainglobe.info/brainreg/tutorial). \\n\\nFor segmentation of bulk structures in 3D space \\n(e.g. injection sites, Neuropixels probes), please see \\n[brainreg-segment](https://github.com/brainglobe/brainreg-segment).\\n\\nThis software is at a very early stage, and was written with our data in mind. \\nOver time we hope to support other data types/formats. If you have any issues, please get in touch [on the forum](https://forum.image.sc/tag/brainglobe) or by \\n[raising an issue](https://github.com/brainglobe/brainreg/issues). \\n\\nIf you have any other questions, \\nplease send an [email](mailto:code@adamltyson.com?subject=brainreg).\\n\\n## Details\\nbrainreg is an update to \\n[amap](https://github.com/SainsburyWellcomeCentre/amap-python) (itself a port \\nof the [original Java software](https://www.nature.com/articles/ncomms11879)) \\nto include multiple registration backends, and to support the many atlases \\nprovided by [bg-atlasapi](https://github.com/brainglobe/bg-atlasapi).\\n\\nThe aim of brainreg is to register the template brain\\n (e.g. from the [Allen Reference Atlas](https://mouse.brain-map.org/static/atlas))\\n  to the sample image. Once this is complete, any other image in the template\\n  space can be aligned with the sample (such as region annotations, for \\n  segmentation of the sample image). The template to sample transformation\\n  can also be inverted, allowing sample images to be aligned in a common \\n  coordinate space.\\n  \\nTo do this, the template and sample images are filtered, and then registered in \\na three step process (reorientation, affine registration, and freeform \\nregistration.) The resulting transform from template to standard space is then\\napplied to the atlas. \\n \\nFull details of the process are in the \\n[original aMAP paper](https://www.nature.com/articles/ncomms11879).\\n![process](https://raw.githubusercontent.com/SainsburyWellcomeCentre/amap-python/master/resources/reg_process.png)\\n**Overview of the registration process**\\n\\n### Citing brainreg\\n\\nIf you find brainreg useful, and use it in your research, please let us know and also cite the preprint:\\n\\n> Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Obenhaus, H. A., Claudi, F., Lenzi, S. C., Branco, T.,  Margrie, T. W. (2021) “Tools for accurate post hoc determination of marker location within whole-brain microscopy images’ bioRxiv, [doi.org/10.1101/2021.05.21.445133](https://doi.org/10.1101/2021.05.21.445133)\\n\\nPlease also cite aMAP (the original pipeline from which this software is based):\\n\\n>Niedworok, C.J., Brown, A.P.Y., Jorge Cardoso, M., Osten, P., Ourselin, S., Modat, M. and Margrie, T.W., (2016). AMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data. Nature Communications. 7, 1–9. https://doi.org/10.1038/ncomms11879\\n\\nLastly, if you can, please cite the BrainGlobe Atlas API that provided the atlas:\\n\\n>Claudi, F., Petrucco, L., Tyson, A. L., Branco, T., Margrie, T. W. and Portugues, R. (2020). BrainGlobe Atlas API: a common interface for neuroanatomical atlases. Journal of Open Source Software, 5(54), 2668, https://doi.org/10.21105/joss.02668\\n\\n**Don't forget to cite the developers of the atlas that you used (e.g. the Allen Brain Atlas)!**\\n\\n\\n\",\n",
       " 'brainreg-segment': \"[![Python Version](https://img.shields.io/pypi/pyversions/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment)\\n[![PyPI](https://img.shields.io/pypi/v/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment)\\n[![Wheel](https://img.shields.io/pypi/wheel/brainreg-segment.svg)](https://pypi.org/project/brainreg-segment)\\n[![Development Status](https://img.shields.io/pypi/status/brainreg-segment.svg)](https://github.com/brainglobe/brainreg-segment)\\n[![Tests](https://img.shields.io/github/workflow/status/brainglobe/brainreg-segment/tests)](\\n    https://github.com/brainglobe/brainreg-segment/actions)\\n[![Coverage Status](https://coveralls.io/repos/github/brainglobe/brainreg-segment/badge.svg?branch=master)](https://coveralls.io/github/brainglobe/brainreg-segment?branch=master)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\\n[![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)\\n\\n# brainreg-segment\\nSegmentation of 1/2/3D brain structures in a common anatomical space\\n\\n`brainreg-segment` is a companion to [`brainreg`](https://github.com/brainglobe/brainreg) allowing manual segmentation of regions/objects within the brain (e.g. injection sites, probes etc.) allowing for automated analysis of brain region distribution, and visualisation (e.g. in [brainrender](https://github.com/BrancoLab/brainrender)).\\n\\n## Installation\\n\\nbrainreg-segment comes bundled with [`brainreg`](https://github.com/brainglobe/brainreg), so see the [brainreg installation instructions](https://docs.brainglobe.info/brainreg/installation). \\n\\nbrainreg-segment can be installed on it's own (`pip install brainreg-segment`), but you will need to register your data with brainreg first. \\n\\n## Usage\\n\\nSee [user guide](https://docs.brainglobe.info/brainreg-segment/user-guide).\\n\\nIf you have any questions, head over to the [image.sc forum](https://forum.image.sc/tag/brainglobe).\\n\\n### Citing brainreg-segment\\n\\nIf you find brainreg-segment useful, and use it in your research, please let us know and also cite the preprint:\\n\\n> Tyson, A. L., V&eacute;lez-Fort, M.,  Rousseau, C. V., Cossell, L., Tsitoura, C., Obenhaus, H. A., Claudi, F., Lenzi, S. C., Branco, T.,  Margrie, T. W. (2021) “Tools for accurate post hoc determination of marker location within whole-brain microscopy images’ bioRxiv, [doi.org/10.1101/2021.05.21.445133](https://doi.org/10.1101/2021.05.21.445133)\\n\\n\\n\",\n",
       " 'cellfinder-napari': \"# cellfinder-napari\\n\\n[![License](https://img.shields.io/pypi/l/cellfinder-napari.svg?color=green)](https://github.com/napari/cellfinder-napari/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/cellfinder-napari.svg?color=green)](https://pypi.org/project/cellfinder-napari)\\n[![Python Version](https://img.shields.io/pypi/pyversions/cellfinder-napari.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brainglobe/cellfinder-napari/workflows/tests/badge.svg)](https://github.com/brainglobe/cellfinder-napari/actions)\\n[![codecov](https://codecov.io/gh/brainglobe/cellfinder-napari/branch/master/graph/badge.svg)](https://codecov.io/gh/brainglobe/cellfinder-napari)\\n[![Downloads](https://pepy.tech/badge/cellfinder-napari)](https://pepy.tech/project/cellfinder-napari)\\n[![Wheel](https://img.shields.io/pypi/wheel/cellfinder.svg)](https://pypi.org/project/cellfinder)\\n[![Development Status](https://img.shields.io/pypi/status/cellfinder-napari.svg)](https://github.com/brainglobe/cellfinder-napari)\\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)\\n[![Contributions](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](https://docs.brainglobe.info/cellfinder/contributing)\\n[![Website](https://img.shields.io/website?up_message=online&url=https%3A%2F%2Fcellfinder.info)](https://cellfinder.info)\\n[![Twitter](https://img.shields.io/twitter/follow/brain_globe?style=social)](https://twitter.com/brain_globe)\\n\\n### Efficient cell detection in large images (e.g. whole mouse brain images)\\n\\nThis package implements the cell detection algorithm from \\n[Tyson, Rousseau & Niedworok et al. (2021)](https://www.biorxiv.org/content/10.1101/2020.10.21.348771v2) \\nfor [napari](https://napari.org/index.html), based on the \\n[cellfinder-core](https://github.com/brainglobe/cellfinder-core) package.\\n\\nThis algorithm can also be used within the original \\n[cellfinder](https://github.com/brainglobe/cellfinder) software for \\nwhole-brain microscopy analysis.\\n\\n----\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder-napari/master/resources/cellfinder-napari.gif)\\n\\n**Visualising detected cells in the cellfinder napari plugin**\\n\\n----\\n## Instructions\\n\\n### Installation\\nOnce you have [installed napari](https://napari.org/index.html#installation). \\nYou can install napari either through the napari plugin installation tool, or \\ndirectly from PyPI with:\\n```bash\\npip install cellfinder-napari\\n```\\n\\n### Usage\\nFull documentation can be \\nfound [here](https://docs.brainglobe.info/cellfinder-napari). \\n\\nThis software is at a very early stage, and was written with our data in mind. \\nOver time we hope to support other data types/formats. If you have any \\nquestions or issues, please get in touch by \\n[email](mailto:code@adamltyson.com?subject=cellfinder-napari), \\n[on the forum](https://forum.image.sc/tag/brainglobe) or by \\n[raising an issue](https://github.com/brainglobe/cellfinder-napari/issues).\\n\\n\\n---\\n## Illustration\\n\\n### Introduction\\ncellfinder takes a stitched, but otherwise raw dataset with at least \\ntwo channels:\\n * Background channel (i.e. autofluorescence)\\n * Signal channel, the one with the cells to be detected:\\n\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/raw.png)\\n**Raw coronal serial two-photon mouse brain image showing labelled cells**\\n\\n\\n### Cell candidate detection\\nClassical image analysis (e.g. filters, thresholding) is used to find \\ncell-like objects (with false positives):\\n\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/detect.png)\\n**Candidate cells (including many artefacts)**\\n\\n\\n### Cell candidate classification\\nA deep-learning network (ResNet) is used to classify cell candidates as true \\ncells or artefacts:\\n\\n![raw](https://raw.githubusercontent.com/brainglobe/cellfinder/master/resources/classify.png)\\n**Cassified cell candidates. Yellow - cells, Blue - artefacts**\\n\\n## Citing cellfinder\\n\\nIf you find this plugin useful, and use it in your research, please cite the preprint outlining the cell detection algorithm:\\n> Tyson, A. L., Rousseau, C. V., Niedworok, C. J., Keshavarzi, S., Tsitoura, C., Cossell, L., Strom, M. and Margrie, T. W. (2021) “A deep learning algorithm for 3D cell detection in whole mouse brain image datasets’ PLOS Computational Biology, 17(5), e1009074\\n[https://doi.org/10.1371/journal.pcbi.1009074](https://doi.org/10.1371/journal.pcbi.1009074)\\n\\n\\n**If you use this, or any other tools in the brainglobe suite, please\\n [let us know](mailto:code@adamltyson.com?subject=cellfinder-napari), and \\n we'd be happy to promote your paper/talk etc.**\\n\\n\\n\",\n",
       " 'cellpose-napari': '# cellpose-napari <img src=\"docs/_static/favicon.ico\" width=\"50\" title=\"cellpose\" alt=\"cellpose\" align=\"right\" vspace = \"50\">\\n\\n[![Documentation Status](https://readthedocs.org/projects/cellpose-napari/badge/?version=latest)](https://cellpose-napari.readthedocs.io/en/latest/?badge=latest)\\n[![tests](https://github.com/mouseland/cellpose-napari/workflows/tests/badge.svg)](https://github.com/mouseland/cellpose-napari/actions)\\n[![codecov](https://codecov.io/gh/Mouseland/cellpose-napari/branch/main/graph/badge.svg)](https://codecov.io/gh/MouseLand/cellpose-napari)\\n[![PyPI version](https://badge.fury.io/py/cellpose-napari.svg)](https://badge.fury.io/py/cellpose-napari)\\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/cellpose-napari)](https://pypistats.org/packages/cellpose-napari)\\n[![Python version](https://img.shields.io/pypi/pyversions/cellpose-napari)](https://pypistats.org/packages/cellpose-napari)\\n[![License](https://img.shields.io/pypi/l/cellpose-napari.svg?color=green)](https://github.com/mouseland/cellpose-napari/raw/master/LICENSE)\\n[![Contributors](https://img.shields.io/github/contributors-anon/MouseLand/cellpose-napari)](https://github.com/MouseLand/cellpose-napari/graphs/contributors)\\n[![website](https://img.shields.io/website?url=https%3A%2F%2Fwww.cellpose.org)](https://www.cellpose.org)\\n[![GitHub stars](https://img.shields.io/github/stars/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/)\\n[![GitHub forks](https://img.shields.io/github/forks/MouseLand/cellpose-napari?style=social)](https://github.com/MouseLand/cellpose-napari/)\\n\\na napari plugin for anatomical segmentation of general cellular images\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\nThe plugin code was written by Carsen Stringer, and the cellpose code was written by Carsen Stringer and Marius Pachitariu. To learn about Cellpose, read the [**paper**](https://t.co/kBMXmPp3Yn?amp=1) or watch this [**talk**](https://t.co/JChCsTD0SK?amp=1). \\n\\nFor support with the plugin, please open an [issue](https://github.com/MouseLand/cellpose-napari/issues). For support with cellpose, please open an [issue](https://github.com/MouseLand/cellpose/issues) on the cellpose repo. \\n\\n\\nIf you use this plugin please cite the [paper](https://www.nature.com/articles/s41592-020-01018-x):\\n::\\n\\n      @article{stringer2021cellpose,\\n      title={Cellpose: a generalist algorithm for cellular segmentation},\\n      author={Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},\\n      journal={Nature Methods},\\n      volume={18},\\n      number={1},\\n      pages={100--106},\\n      year={2021},\\n      publisher={Nature Publishing Group}\\n      }\\n\\n\\n![cellpose-napari_plugin](docs/_static/napari_main_demo_fast_small.gif?raw=true \"cellpose-napari\")\\n\\n## Installation\\n\\nInstall an [Anaconda](https://www.anaconda.com/download/) distribution of Python -- Choose **Python 3** and your operating system. Note you might need to use an anaconda prompt if you did not add anaconda to the path. \\n\\nYou can install `cellpose-napari` via [pip]:\\n\\n    pip install cellpose-napari\\n\\nIf install fails in your base environment, create a new environment:\\n1. Download the [`environment.yml`](https://github.com/MouseLand/cellpose-napari/blob/master/environment.yml?raw=true) file from the repository. You can do this by cloning the repository, or copy-pasting the text from the file into a text document on your local computer.\\n2. Open an anaconda prompt / command prompt with `conda` for **python 3** in the path\\n3. Change directories to where the `environment.yml` is and run `conda env create -f environment.yml`\\n4. To activate this new environment, run `conda activate cellpose-napari`\\n5. You should see `(cellpose-napari)` on the left side of the terminal line. \\n\\nIf you have **issues** with cellpose installation, see the [cellpose docs](https://cellpose.readthedocs.io/en/latest/installation.html) for more details, and then if the suggestions fail, open an issue.\\n\\n### Upgrading software\\n\\nYou can upgrade the plugin with\\n~~~\\npip install cellpose-napari --upgrade\\n~~~\\n\\nand you can upgrade cellpose with\\n~~~\\npip install cellpose --upgrade\\n~~~\\n\\n### GPU version (CUDA) on Windows or Linux\\n\\nIf you plan on running many images, you may want to install a GPU version of *torch* (if it isn\\'t already installed).\\n\\nBefore installing the GPU version, remove the CPU version:\\n~~~\\npip uninstall torch\\n~~~\\n\\nFollow the instructions [here](https://pytorch.org/get-started/locally/) to determine what version to install. The Anaconda install is recommended along with CUDA version 10.2. For instance this command will install the 10.2 version on Linux and Windows (note the `torchvision` and `torchaudio` commands are removed because cellpose doesn\\'t require them):\\n\\n~~~\\nconda install pytorch cudatoolkit=10.2 -c pytorch\\n~~~~\\n\\nWhen upgrading GPU Cellpose in the future, you will want to ignore dependencies (to ensure that the pip version of torch does not install):\\n~~~\\npip install --no-deps cellpose --upgrade\\n~~~\\n\\n### Installation of github version\\n\\nFollow steps from above to install the dependencies. In the github repository, run `pip install -e .` and the github version will be installed. If you want to go back to the pip version of cellpose-napari, then say `pip install cellpose-napari`.\\n\\n\\n## Running the software\\n\\n\\nOpen napari with the cellpose-napari dock widget open\\n```\\nnapari -w cellpose-napari\\n```\\n\\nThere is sample data in the File menu, or get started with your own images!\\n\\n### Detailed usage [documentation](https://cellpose-napari.readthedocs.io/).\\n\\n## Contributing\\n\\nContributions are very welcome. Tests are run with pytest.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"cellpose-napari\" is free and open source software.\\n\\n## Dependencies\\ncellpose-napari relies on the following excellent packages (which are automatically installed with conda/pip if missing):\\n- [napari](https://napari.org)\\n- [magicgui](https://napari.org/magicgui/)\\n\\ncellpose relies on the following excellent packages (which are automatically installed with conda/pip if missing):\\n- [torch](https://pytorch.org/)\\n- [numpy](http://www.numpy.org/) (>=1.16.0)\\n- [numba](http://numba.pydata.org/numba-doc/latest/user/5minguide.html)\\n- [scipy](https://www.scipy.org/)\\n- [natsort](https://natsort.readthedocs.io/en/master/)\\n- [tifffile](https://pypi.org/project/tifffile/)\\n- [opencv](https://opencv.org/)\\n\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n\\n',\n",
       " 'devbio-napari': '# devbio-napari\\n\\n[![License](https://img.shields.io/pypi/l/devbio.svg?color=green)](https://github.com/haesleinhuepf/devbio/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/devbio.svg?color=green)](https://pypi.org/project/devbio)\\n[![Python Version](https://img.shields.io/pypi/pyversions/devbio.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/devbio/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/devbio/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/devbio/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/devbio)\\n\\nA collection of napari plugins useful for studying developmental biology consisting of\\n* [aicsimageio]https://github.com/AllenCellModeling/napari-aicsimageio\\n* [Cellpose](https://github.com/MouseLand/cellpose-napari)\\n* [StarDist](https://github.com/stardist/stardist-napari)\\n* [clEsperanto](https://clesperanto.github.io/napari_pyclesperanto_assistant/)\\n* [scikit-image regionprops](https://github.com/haesleinhuepf/napari-skimage-regionprops)\\n* [animation](https://github.com/napari/napari-animation)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `devbio` via [pip]:\\n\\n    pip install napari[]\\n\\n    pip install devbio-napari\\n\\nWindows users should install [pyopencl](https://documen.tician.de/pyopencl/) via conda in **advance**:\\n\\n    conda install -c conda-forge pyopencl\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"devbio\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/haesleinhuepf/devbio/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'elastix-napari': '# Description\\n\\nThis plugin makes the elastix toolbox for rigid and nonrigid registration of images available in napari.\\nelastix is open source software, based on the well-known Insight Segmentation and Registration Toolkit (ITK). The software consists of a collection of algorithms that are commonly used to solve (medical) image registration problems. The modular design of elastix allows the user to quickly configure, test, and compare different registration methods for a specific application.\\n\\n# Who is This For?\\n\\nWith this plugin both 2D and 3D images in all file formats available in ITK can be registered.\\nThe plugin supports various transformations including rigid, affine and bspline.\\n\\nRegistration within the plugin is done based on user defined parameters, but for novice users\\ndefaults for each transformation model are available.\\n\\n# How to Guide\\n\\nLoad the images you want to register into napari and select them in the fixed (or reference) and moving image dropdowns of plugin interface.\\n\\nFor fast and easy registration only the preferred transformation (rigid, affine or bspline) has to be selected (see Transformations section for explanation).\\n\\nFor more advanced registrations the following adjustments can be made in the plugin:\\n\\n- Masks for both the fixed and the moving images can be selected to let elastix only include certain areas in the registration. These masks have to be loaded into napari and selected in the correct mask dropdown menus, which appear when the masks box is ticked.\\n- Point sets for both the fixed and the moving images can de selected to use certain points to aid registration. These point set files have to be .txt files in the following format:\\n\\n  index/point\\\\\\n  #points\\\\\\n  point1 x point1 y [point1 z]\\\\\\n  point2 x point2 y [point2 z]\\n\\n  The first line indicates whether the points are given as “indices” (of the fixed image), or as “points” (in\\n  physical coordinates). The second line stores the number of points that will be specified. After that the\\n  point data is given. For example:\\n\\n  point\\\\\\n  3\\\\\\n  2.32 5.34 -4.12\\\\\\n  -1.56 0.12 9.23\\\\\\n  1.00 7.34 -0.23\\n\\n- An initial transform file that specifies a transform that is applied before the registration is done, can be uploaded as a .txt file. For the latest file and transform formats that are supported, see the [elastix manual](https://elastix.lumc.nl/doxygen/index.html)\\n\\n- For the most common registration parameters adjustments can be made in the plugin GUI\\n\\n- Other, less common registration parameters can be adjusted by uploading custom transform parameter file(s). (Select \\'custom\\' in the preset dropdown).\\n\\n\\n<img width=\"1438\" alt=\"Screenshot 2021-05-12 at 15 07 24\" src=\"https://user-images.githubusercontent.com/33719474/117980045-d6009b00-b333-11eb-9976-f64d34f4f7cc.png\">\\n\\n# Transformations\\n\\nIn the plugin 3 common transformations are available as presets, other transformations can be done with the \\'custom\\' option in the preset dropdown. The plugin then has the ability to upload custom parameter files in which other transformations can be specified.\\n\\nThe three common transformations are:\\n\\n- [Rigid Transform](https://en.wikipedia.org/wiki/Rigid_transformation):\\nAlso known as a Euclidean transformation, this transform preserves the Euclidean\\ndistance between each pair of points on the image. This includes rotation,\\ntranslation and reflection but not scaling or shearing.\\n\\n\\n- [Affine Transform](https://en.wikipedia.org/wiki/Affine_transformation):\\nThis transfrom preserves\\nlines and parallelism, but not necessarily distance and angles. Translation,\\nscaling, similarity, reflection, rotation and shearing are all valid\\naffine transformations.\\n\\n- [BSpline Transform](https://en.wikipedia.org/wiki/B-spline):\\nThis is a deformable transformation that preserves none of the properties mentioned in the transforms describe above.\\n\\n# Getting Help\\nIf you find a bug in the elastix napari plugin, or would like support with using it, please raise an\\nissue on the [GitHub repository](https://github.com/SuperElastix/elastix_napari).\\n\\nFor question specifically about the elastix toolbox we have a [mailing list](https://groups.google.com/forum/#!forum/elastix-imageregistration).\\n\\n# Contributions\\nContributions to the elastix_napari plugin, [itkelastix](https://github.com/InsightSoftwareConsortium/ITKElastix) (the python wrapper) or [elastix](https://github.com/SuperElastix/elastix) (the C++ core) on which the plugin is build, are welcome.\\n',\n",
       " 'misic-napari-plugin': '# misic-napari-plugin\\n\\n[![License](https://img.shields.io/pypi/l/misic-napari-plugin.svg?color=green)](https://github.com/pswap/misic-napari-plugin/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/misic-napari-plugin.svg?color=green)](https://pypi.org/project/misic-napari-plugin)\\n[![Python Version](https://img.shields.io/pypi/pyversions/misic-napari-plugin.svg?color=green)](https://python.org)\\n[![tests](https://github.com/pswap/misic-napari-plugin/workflows/tests/badge.svg)](https://github.com/pswap/misic-napari-plugin/actions)\\n[![codecov](https://codecov.io/gh/pswap/misic-napari-plugin/branch/master/graph/badge.svg)](https://codecov.io/gh/pswap/misic-napari-plugin)\\n\\nsegmentation of bacteria\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `misic-napari-plugin` via [pip]:\\n\\n    pip install misic-napari-plugin\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"misic-napari-plugin\" is free and open source software\\n\\n\\n',\n",
       " 'napari-aicsimageio': '## Features\\n* Supports reading metadata and imaging data for:\\n    * `CZI`\\n    * `OME-TIFF`\\n    * `TIFF`\\n    * Any formats supported by [aicsimageio](https://github.com/AllenCellModeling/aicsimageio)\\n    * Any additional format supported by [imageio](https://github.com/imageio/imageio)\\n\\n### Plugin Variants\\n\\n![screenshot of plugin sorter showing that napari-aicsimageio-in-memory should be placed above napari-aicsimageio-out-of-memory](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/plugin-sorter.png)\\n\\nThere are two variants of this plugin that are added during installation:\\n* `aicsimageio-in-memory`, which reads an image fully into memory\\n* `aicsimageio-out-of-memory`, which delays reading ZYX chunks until required.\\nThis allows for incredible large files to be read and displayed.\\n\\n## Examples of Features\\n\\n#### General Image Reading\\n\\nAll image file formats supported by\\n[aicsimageio](https://github.com/AllenCellModeling/aicsimageio) will be read and all\\nraw data will be available in the napari viewer.\\n\\nIn addition, when reading an OME-TIFF, you can view all OME metadata directly in the\\nnapari viewer thanks to `ome-types`.\\n\\n![screenshot of an OME-TIFF image view, multi-channel, z-stack, with metadata viewer](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/ome-tiff-with-metadata-viewer.png)\\n\\n#### Mosaic CZI Reading\\n\\nWhen reading CZI images, if the image is a mosaic tiled image, `napari-aicsimageio`\\nwill return the reconstructed image:\\n\\n![screenshot of a reconstructed / restitched mosaic tile CZI](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/tiled-czi.png)\\n\\n#### Mosaic LIF Reading\\n\\nWhen reading LIF images, if the image is a mosaic tiled image, `napari-aicsimageio`\\nwill return the reconstructed image:\\n\\n![screenshot of a reconstructed / restitched mosaic tile LIF](https://raw.githubusercontent.com/AllenCellModeling/napari-aicsimageio/main/images/tiled-lif.png)\\n\\n## Citation\\n\\nIf you find `aicsimageio` _(or `napari-aicsimageio`)_ useful, please cite as:\\n\\n> AICSImageIO Contributors (2021). AICSImageIO: Image Reading, Metadata Conversion, and Image Writing for Microscopy Images in Pure Python [Computer software]. GitHub. https://github.com/AllenCellModeling/aicsimageio\\n\\n_Free software: BSD-3-Clause_',\n",
       " 'napari-allencell-segmenter': '# napari-allencell-segmenter\\n\\n[![License](https://img.shields.io/pypi/l/napari-allencell-segmenter.svg?color=green)](https://github.com/AllenCell/napari-allencell-segmenter/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-allencell-segmenter.svg?color=green)](https://pypi.org/project/napari-allencell-segmenter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-allencell-segmenter.svg?color=green)](https://python.org)\\n[![tests](https://github.com/AllenCell/napari-allencell-segmenter/workflows/tests/badge.svg)](https://github.com/AllenCell/napari-allencell-segmenter/actions)\\n[![codecov](https://codecov.io/gh/AllenCell/napari-allencell-segmenter/branch/main/graph/badge.svg)](https://codecov.io/gh/AllenCell/napari-allencell-segmenter)\\n\\n\\nA plugin that enables 3D image segmentation provided by Allen Institute for Cell Science\\n\\nThe Allen Cell & Structure Segmenter plugin for napari provides an intuitive graphical user interface to access the powerful segmentation capabilities of an open source 3D segmentation software package developed and maintained by the Allen Institute for Cell Science (classic workflows only with v1.0). \\u200b[The Allen Cell & Structure Segmenter](allencell.org/segmenter) is a Python-based open source toolkit developed at the Allen Institute for Cell Science for 3D segmentation of intracellular structures in fluorescence microscope images. This toolkit brings together classic image segmentation and iterative deep learning workflows first to generate initial high-quality 3D intracellular structure segmentations and then to easily curate these results to generate the ground truths for building robust and accurate deep learning models. The toolkit takes advantage of the high replicate 3D live cell image data collected at the Allen Institute for Cell Science of over 30 endogenous fluorescently tagged human induced pluripotent stem cell (hiPSC) lines. Each cell line represents a different intracellular structure with one or more distinct localization patterns within undifferentiated hiPS cells and hiPSC-derived cardiomyocytes.\\n\\nMore details about Segmenter can be found at https://allencell.org/segmenter\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\n### Option 1 (recommended):\\n\\nAfter you installed the lastest version of napari, you can go to \"Plugins\" --> \"Install/Uninstall Package(s)\". Then, you will be able to see all available napari plugins and you can find us by name `napari-allencell-segmenter`. Just click the \"install\" button to install the Segmenter plugin.\\n\\n### Option 2:\\n\\nYou can also install `napari-allencell-segmenter` via [pip]:\\n\\n    pip install napari-allencell-segmenter\\n\\n## Quick Start\\n\\nIn the current version, there are two parts in the plugin: **workflow editor** and **batch processing**. The **workflow editor** allows users adjusting parameters in all the existing workflows in the lookup table, so that the workflow can be optimized on users\\' data. The adjusted workflow can be saved and then applied to a large batch of files using the **batch processing** part of the plugin. \\n\\n1. Open a file in napari (the plugin is able to support multi-dimensional data in .tiff, .tif. ome.tif, .ome.tiff, .czi)\\n2. Start the plugin (open napari, go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"workflow editor\")\\n3. Select the image and channel to work on\\n4. Select a workflow based on the example image and target segmentation based on user\\'s data. Ideally, it is recommend to start with the example with very similar morphology as user\\'s data.\\n5. Click \"Run All\" to execute the whole workflow on the sample data.\\n6. Adjust the parameters of steps, based on the intermediate results. For instruction on the details on each function and the effect of each parameter, click the tooltip button. A complete list of all functions can be found [here](https://github.com/AllenCell/aics-segmentation/blob/main/aicssegmentation/structure_wrapper_config/function_params.md)\\n7. Click \"Run All\" again after adjusting the parameters and repeat step 6 and 7 until the result is satisfactory.\\n8. Save the workflow\\n9. Close the plugin and open the **batch processing** part by (go to \"Plugins\" --> \"napari-allencell-segmenter\" --> \"batch processing\")\\n10. Load the customized workflow (or an off-the-shelf workflow) json file\\n11. Load the folder with all the images to process\\n12. Click \"Run\"\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-allencell-segmenter\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/AllenCell/napari-allencell-segmenter/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-animation': '# napari-animation (WIP under active development)\\n\\n[![License](https://img.shields.io/pypi/l/napari-animation.svg?color=green)](https://github.com/napari/napari-animation/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-animation.svg?color=green)](https://pypi.org/project/napari-animation)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-animation.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sofroniewn/napari-animation/workflows/tests/badge.svg)](https://github.com/sofroniewn/napari-animation/actions)\\n[![codecov](https://codecov.io/gh/sofroniewn/napari-animation/branch/master/graph/badge.svg)](https://codecov.io/gh/sofroniewn/napari-animation)\\n\\n**napari-animation** is a plugin for making animations in [napari].\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\nIt is built off of great work from @guiwitz in [naparimovie](https://github.com/guiwitz/naparimovie) which was initially submitted to napari in [PR#851](https://github.com/napari/napari/pull/780).\\n\\n----------------------------------\\n## Overview\\n\\n**napari-animation** provides a framework for the creation of animations in napari and features:\\n- an easy to use GUI for interactive creation of animations\\n- Python tools for programmatic creation of animations\\n\\nThis plugin is currently pre-release and under active development. APIs are likely to change before it\\'s first 0.0.1 release,\\nbut feedback and contributions are welcome.\\n\\n## Installation\\n\\nYou can clone this repository with install locally with\\n\\n    pip install -e .\\n\\n## Examples\\nExamples can be found in our [examples](examples) folder. Simple examples for both interactive and headless \\nuse of the plugin follow.\\n\\n### Interactive\\n**napari-animation** can be used interactively by creating an `AnimationWidget` from a napari `Viewer` and adding it to\\nthe viewer as a dock widget.\\n\\n```python\\nfrom napari_animation import AnimationWidget\\n\\nanimation_widget = AnimationWidget(viewer)\\nviewer.window.add_dock_widget(animation_widget, area=\\'right\\')\\n```\\n\\n![AnimationWidget image](resources/screenshot-animation-widget.png)\\n\\n### Headless\\n**napari-animation** can also be run headless, allowing for reproducible, scripted creation of animations.\\n\\n```python\\nfrom napari_animation import Animation\\n\\nanimation = Animation(viewer)\\n\\nviewer.dims.ndisplay = 3\\nviewer.camera.angles = (0.0, 0.0, 90.0)\\nanimation.capture_keyframe()\\nviewer.camera.zoom = 2.4\\nanimation.capture_keyframe()\\nviewer.camera.angles = (-7.0, 15.7, 62.4)\\nanimation.capture_keyframe(steps=60)\\nviewer.camera.angles = (2.0, -24.4, -36.7)\\nanimation.capture_keyframe(steps=60)\\nviewer.reset_view()\\nviewer.camera.angles = (0.0, 0.0, 90.0)\\nanimation.capture_keyframe()\\nanimation.animate(\\'demo.mov\\', canvas_only=False)\\n```\\n\\n## Is everything animate-able?\\n\\nUnfortunately, not yet! Currently differences in the following objects are tracked by the `Animation` class\\n\\n- `Viewer.camera`\\n- `Viewer.dims`\\n- `Layer.scale`\\n- `Layer.translate`\\n- `Layer.rotate`\\n- `Layer.shear`\\n- `layer.opacity`\\n- `Layer.blending`\\n- `Layer.visible`\\n\\nSupport for more layer attributes will be added in future releases.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests and additional infrastructure are being setup.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-animation\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sofroniewn/napari-animation/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-arboretum': \" <!--[![Downloads](https://pepy.tech/badge/napari-arboretum)](https://pepy.tech/project/napari-arboretum)-->\\n[![License](https://img.shields.io/pypi/l/napari-arboretum.svg?color=green)](https://github.com/quantumjot/napari-arboretum/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-arboretum.svg?color=green)](https://pypi.org/project/napari-arboretum)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-arboretum.svg?color=green)](https://python.org)\\n[![tests](https://github.com/quantumjot/arboretum/workflows/tests/badge.svg)](https://github.com/quantumjot/arboretum/actions)\\n[![codecov](https://codecov.io/gh/quantumjot/arboretum/branch/master/graph/badge.svg)](https://codecov.io/gh/quantumjot/arboretum)\\n\\n# Arboretum\\n\\n\\n### Overview\\n\\nA dockable widget for [Napari](https://github.com/napari) for visualizing cell lineage trees.\\n\\n\\nFeatures:\\n+ Lineage tree plot widget\\n+ Integration with [btrack](https://github.com/quantumjot/BayesianTracker)\\n\\nThis project has changed considerably. The `Tracks` layer, originally developed for this plugin, is now an official layer type in napari. Read the napari documentation here:  \\n https://napari.org/api/stable/napari.layers.Tracks.html#napari.layers.Tracks\\n\\n\\nTo view the legacy version of this plugin, visit the legacy branch:  \\nhttps://github.com/quantumjot/arboretum/tree/v1-legacy\\n\\n[![LineageTree](./examples/arboretum.png)]()  \\n*Automated cell tracking and lineage tree reconstruction*.\\n\\n---  \\n\\n\\n\\n\\n\\n### Installation\\n\\nWe recommend that you first install napari. Detailed instructions are here: https://github.com/napari/napari.\\n\\n```sh\\npip install 'napari[all]'\\n```\\n\\nYou can install arboretum directly from napari (using the `Plugins > Install/Uninstall Packages(s)...` menu) and searching for `napari-arboretum`.\\n\\nAlternatively, you can install directly from source:\\n\\n```sh\\ngit clone https://github.com/quantumjot/arboretum.git\\ncd arboretum\\npip install -e .\\n```\\n\\n### Usage\\n\\nOnce installed, Arboretum will be visible in the `Plugins > Add Dock Widget > napari-arboretum` menu in napari.  To visualize a lineage tree, click on one of the tracks in a napari `Tracks` layer.\\n\\nYou can use *btrack* to generate tracks from your image data. See the example notebook here:  \\nhttps://github.com/quantumjot/BayesianTracker/blob/master/examples/segmentation_to_btrack_to_napari.ipynb\\n\\n\\n\\n---\\n\\n#### TODO:\\n+ [ ] Highlight cells in the viewer from the lineage tree view\\n+ [ ] Visualize merges\\n+ [ ] Color trees by properties\\n\\n\\n---\\n\\n\\n\",\n",
       " 'napari-autolign': '# napari-autolign\\n\\n[![License](https://img.shields.io/pypi/l/napari-autolign.svg?color=green)](https://github.com/krentzd/napari-autolign/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-autolign.svg?color=green)](https://pypi.org/project/napari-autolign)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-autolign.svg?color=green)](https://python.org)\\n[![tests](https://github.com/krentzd/napari-autolign/workflows/tests/badge.svg)](https://github.com/krentzd/napari-autolign/actions)\\n[![codecov](https://codecov.io/gh/krentzd/napari-autolign/branch/master/graph/badge.svg)](https://codecov.io/gh/krentzd/napari-autolign)\\n\\nA plugin for registering multimodal image volumes based on common segmented structures of interest with point-clouds.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-autolign` via [pip]:\\n\\n    pip install napari-autolign\\n\\nWhen installing autolign on a Windows machine you may encounter the following error:\\n\\n    error Microsoft Visual C++ 14.0 is required\\n\\nEnsure that [Visual Studios C++ 14.00](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16) is installed\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"napari-autolign\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/krentzd/napari-autolign/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/',\n",
       " 'napari-bioformats': '# napari-bioformats\\n\\n[![License](https://img.shields.io/pypi/l/napari-bioformats.svg?color=green)](https://github.com/napari/napari-bioformats/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-bioformats.svg?color=green)](https://pypi.org/project/napari-bioformats)\\n[![Conda](https://img.shields.io/conda/v/conda-forge/napari-bioformats)](https://anaconda.org/conda-forge/napari-bioformats)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-bioformats.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-bioformats/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-bioformats/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-bioformats/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-bioformats)\\n\\nBioformats plugin for napari using\\n[pims-bioformats](http://soft-matter.github.io/pims/v0.5/bioformats.html)\\n\\n----------------------------------\\n\\n## Use this plugin as a fallback!\\n\\nAnyone coming to napari from the Fiji/ImageJ world will likely be aware of the\\n_incredible_ [Bio-Formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/index.html)\\nlibrary.  A heroic effort, built over years, to read\\n[more than a 100 file formats](https://docs.openmicroscopy.org/bio-formats/6.6.1/supported-formats.html).  Naturally, we want some of that goodness for `napari` ... hence this plugin.\\n\\n**However:** it\\'s important to note that this plugin _still_\\nrequires having a java runtime engine installed.  This is easy enough to do\\n(the plugin will ask to install it for you if you\\'re in a `conda` environment), but\\nit definitely makes for a more complicated environment setup, it\\'s not very\\n\"pythonic\", and the performance will likely not feel as snappy as a native \"pure\"\\npython module.\\n\\nSo, before you reflexively install this plugin to fill that bio-formats\\nsized hole in your python heart, consider trying some of the other pure-python\\nplugins designed to read your format of interest:\\n\\n- **Zeiss (.czi)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio), [napari-czifile2](https://github.com/BodenmillerGroup/napari-czifile2)\\n- **Nikon (.nd2)**: [napari-nikon-nd2](https://github.com/cwood1967/napari-nikon-nd2), [nd2-dask](https://github.com/DragaDoncila/nd2-dask)\\n- **Leica (.lif)**: [napari-aicsimageio](https://github.com/AllenCellModeling/napari-aicsimageio)\\n- **Olympus (.oif)**: no plugin?  (but see [oiffile](https://pypi.org/project/oiffile/) )\\n- **DeltaVision (.dv, .mrc)**: [napari-dv](https://github.com/tlambert03/napari-dv)\\n\\n> *if you have a pure-python reader for a bio-formats-supported file format that\\nyou\\'d like to see added to this list, please open an issue*\\n\\n## Installation\\n\\nThe easiest way to install `napari-bioformats` is via [conda], from the\\n[conda-forge] channel:\\n\\n    conda install -c conda-forge napari-bioformats\\n\\nIt is also possible to install via [pip], but you will need to have a working\\nJVM installed, and may need to set the `JAVA_HOME` environment variable\\n\\n    pip install napari-bioformats\\n\\n### First Usage\\n\\nThe first time you attempt to open a file with napari-bioformats, you will\\nlikely notice a long delay as pims downloads the `loci_tools.jar` (speed will\\ndepend on your internet connection). Subsequent files should open more quickly.\\n\\n## License\\n\\nDistributed under the terms of the [GPLv3] license,\\n\"napari-bioformats\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n_This [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template._\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[GPLv3]: https://opensource.org/licenses/GPL-3.0\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/tlambert03/napari-bioformats/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[conda]: https://docs.conda.io/en/latest/\\n[conda-forge]: https://conda-forge.org\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-brightness-contrast': '# napari-brightness-contrast\\n\\n[![License](https://img.shields.io/pypi/l/napari-brightness-contrast.svg?color=green)](https://github.com/haesleinhuepf/napari-brightness-contrast/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-brightness-contrast.svg?color=green)](https://pypi.org/project/napari-brightness-contrast)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-brightness-contrast.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-brightness-contrast/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-brightness-contrast/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-brightness-contrast)\\n\\nAdvanced layer histogram visualization options, e.g. for brightness / contrast\\n![](https://github.com/haesleinhuepf/napari-brightness-contrast/blob/main/docs/images/napari-brightness-contrast3.gif?raw=true)\\n\\nNote: This will not work for big image data at the moment. \\nIf the user interface feels slow, consider installing [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) to speed it up.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-brightness-contrast` via [pip]:\\n\\n    pip install napari-brightness-contrast\\n\\n## Contributing\\n\\nContributions are very welcome.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-brightness-contrast\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/haesleinhuepf/napari-brightness-contrast/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-btrack-reader': '# napari-btrack-reader\\n\\n[![License](https://img.shields.io/pypi/l/napari-btrack-reader.svg?color=green)](https://github.com/napari/napari-btrack-reader/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-btrack-reader.svg?color=green)](https://pypi.org/project/napari-btrack-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-btrack-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/quantumjot/napari-btrack-reader/workflows/tests/badge.svg)](https://github.com/quantumjot/napari-btrack-reader/actions)\\n[![codecov](https://codecov.io/gh/quantumjot/napari-btrack-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/quantumjot/napari-btrack-reader)\\n\\nA plugin to load btrack files\\n\\n----------------------------------\\n\\nThis plugin reads tracking data generated by BayesianTracker (btrack).\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-btrack-reader` via [pip]:\\n\\n    pip install napari-btrack-reader\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-btrack-reader\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/quantumjot/napari-btrack-reader/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-checkerboard': '# napari-checkerboard\\n\\n[![License](https://img.shields.io/pypi/l/napari-checkerboard.svg?color=green)](https://github.com/ViktorvdValk/napari-checkerboard/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-checkerboard.svg?color=green)](https://pypi.org/project/napari-checkerboard)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-checkerboard.svg?color=green)](https://python.org)\\n[![tests](https://github.com/ViktorvdValk/napari-checkerboard/workflows/tests/badge.svg)](https://github.com/ViktorvdValk/napari-checkerboard/actions)\\n[![codecov](https://codecov.io/gh/ViktorvdValk/napari-checkerboard/branch/master/graph/badge.svg)](https://codecov.io/gh/ViktorvdValk/napari-checkerboard)\\n\\nCompare two images with the itk checkerboard filter\\n\\n\\n<img width=\"1430\" alt=\"Screenshot 2021-05-12 at 15 03 17\" src=\"https://user-images.githubusercontent.com/33719474/117979519-48bd4680-b333-11eb-874c-d9ec09681d93.png\">\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-checkerboard` via [pip]:\\n\\n    pip install napari-checkerboard\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\"napari-checkerboard\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/ViktorvdValk/napari-checkerboard/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-clemreg': '# napari-clemreg\\n\\n[![License](https://img.shields.io/pypi/l/napari-clemreg.svg?color=green)](https://github.com/krentzd/napari-clemreg/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-clemreg.svg?color=green)](https://pypi.org/project/napari-clemreg)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-clemreg.svg?color=green)](https://python.org)\\n[![tests](https://github.com/krentzd/napari-clemreg/workflows/tests/badge.svg)](https://github.com/krentzd/napari-clemreg/actions)\\n[![codecov](https://codecov.io/gh/krentzd/napari-clemreg/branch/master/graph/badge.svg)](https://codecov.io/gh/krentzd/napari-clemreg)\\n\\nA plugin for registering multimodal image volumes based on common segmented structures of interest with point-clouds.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-clemreg` via [pip]:\\n\\n    pip install napari-clemreg\\n\\nWhen installing clemreg on a Windows machine you may encounter the following error:\\n\\n    error Microsoft Visual C++ 14.0 is required\\n\\nEnsure that [Visual Studios C++ 14.00](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16) is installed\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"napari-clemreg\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/krentzd/napari-clemreg/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/',\n",
       " 'napari-compressed-labels-io': '# napari-compressed-labels-io\\n\\n[![License](https://img.shields.io/pypi/l/napari-compressed-labels-io.svg?color=green)](https://github.com/DragaDoncila/napari-compressed-labels-io/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-compressed-labels-io.svg?color=green)](https://pypi.org/project/napari-compressed-labels-io)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-compressed-labels-io.svg?color=green)](https://python.org)\\n[![tests](https://github.com/DragaDoncila/napari-compressed-labels-io/workflows/tests/badge.svg)](https://github.com/DragaDoncila/napari-compressed-labels-io/actions)\\n[![codecov](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io/branch/master/graph/badge.svg)](https://codecov.io/gh/DragaDoncila/napari-compressed-labels-io)\\n\\n\\n## Description\\n\\nThis napari plugin provides readers and writers for labels and their corresponding image layers into zarr format for compression and portability. Each reader/writer pair supports a round trip of saving and loading image and labels layers.\\n\\n## Writers\\nTwo writers are provided by this plugin, each with its own reader.\\n\\n### `labels_to_zarr`\\nThis writer is an alternative to napari\\'s default label writer and will write an entire labels layer, regardless of its dimensions, into a single zarr file. This writer provides the best compression option and its associated reader `get_zarr_labels` will read the layer back into napari.\\n\\nThis writer will be called when the user tries to save a selected labels layer into a path ending with .zarr\\n\\n### `label_image_pairs_to_zarr`\\nThis writer will save 3-dimensional labels and image layers from the viewer into individual zarrs for portability and convenience. For example, given one labels and one image layer of the shape (10, 200, 200) saved to my_stacks.zarr, 10 subdirectories will be created, each with two zarrs inside of shape (200, 200) corresponding to the labels and image layer.\\n\\nThis writer allows users to load stacks of associated images, label them, and then quickly save these stacks out into individual slices for easy loading, viewing and interaction. Its associated reader supports the loading into napari of the whole stack, all layers at one slice of the stack, and an individual layer of a given slice of the stack.\\n\\nThe writer currently supports only 3D layers, with the exception of RGB images of the form (z, y, x, 3), which are also supported.\\n\\n\\n## Readers\\n\\nTwo readers are provided by this plugin for loading the formats saved by each writer. These are detailed below.\\n\\n### `get_zarr_labels`\\n\\nThis reader will open any zarr file with a .zarray at the top level in `path` as a labels layer. This is to be used in conjunction with `labels_to_zarr`.\\n\\n\\n### `get_label_image_stack`\\n\\nThis reader will open any zarr containing a `.zmeta` file as layers into napari. Depending on what is being opened, the reader will either load a full stack of labels and images, one slice of a stack of images and labels or an individual layer within a slice. This is to be used in conjunction with `label_image_pairs_to_zarr`.\\n\\n## .zmeta\\n\\nThis metadata file contains information about the layer types in the stack and in each individual slice, as well as the number of image/label slices. This allows the reader plugin to load the correct layer types with appropriate names both at a stack level and at the individual slice level.\\n\\n### An example .zmeta specification\\n\\n```json\\n{\\n    \"meta\": {\\n        \"stack\": 7                               # number of slices in the entire stack (1 for an individual slice, 0 for a layer within a slice)\\n    },\\n    \"data\": {\\n        \"image\" : [                              # all image layers must be listed here\\n            {\\n                \"name\": \"leaves_example_data\",\\n                \"shape\": [790, 790, 3],\\n                \"dtype\": \"uint8\",\\n                \"rgb\": true                      # where rgb is false the image will be loaded as greyscale (colormap support has not yet been implemented)\\n            }\\n        ],\\n        \"labels\" : [\\n            {\\n                \"name\": \"oak\",\\n                \"shape\": [790, 790],\\n                \"dtype\": \"int64\"\\n            },\\n            {\\n                \"name\": \"bg\",\\n                \"shape\": [790, 790],\\n                \"dtype\": \"int64\"\\n            }\\n        ]\\n    }\\n}\\n\\n```\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-compressed-labels-io` via [pip]:\\n\\n    pip install napari-compressed-labels-io\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"napari-compressed-labels-io\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/DragaDoncila/napari-compressed-labels-io/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-console': '# napari-console (WIP, under active development)\\n\\n[![License](https://img.shields.io/pypi/l/napari-console.svg?color=green)](https://github.com/napari/napari-console/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-console.svg?color=green)](https://pypi.org/project/napari-console)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-console.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sofroniewn/napari-console/workflows/tests/badge.svg)](https://github.com/sofroniewn/napari-console/actions)\\n[![codecov](https://codecov.io/gh/sofroniewn/napari-console/branch/master/graph/badge.svg)](https://codecov.io/gh/sofroniewn/napari-console)\\n\\nA plugin that adds a console to napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-console` via [pip]:\\n\\n    pip install napari-console\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-console\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sofroniewn/napari-console/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-czifile2': '# napari-czifile2\\n\\n<a href=\"https://pypi.org/project/napari-czifile2/\">\\n    <img src=\"https://img.shields.io/pypi/v/napari-czifile2\" alt=\"PyPI\" />\\n</a>\\n<a href=\"https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md\">\\n    <img src=\"https://img.shields.io/pypi/l/napari-czifile2\" alt=\"License\" />\\n</a>\\n<a href=\"https://www.python.org/\">\\n    <img src=\"https://img.shields.io/pypi/pyversions/napari-czifile2\" alt=\"Python\" />\\n</a>\\n<a href=\"https://github.com/BodenmillerGroup/napari-czifile2/issues\">\\n    <img src=\"https://img.shields.io/github/issues/BodenmillerGroup/napari-czifile2\" alt=\"Issues\" />\\n</a>\\n<a href=\"https://github.com/BodenmillerGroup/napari-czifile2/pulls\">\\n    <img src=\"https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-czifile2\" alt=\"Pull requests\" />\\n</a>\\n\\nCarl Zeiss Image (.czi) file type support for napari\\n\\nOpen .czi files and interactively view scenes co-registered in the machine\\'s coordinate system using napari\\n\\n## Installation\\n\\nYou can install napari-czifile2 via [pip](https://pypi.org/project/pip/):\\n\\n    pip install napari-czifile2\\n\\n## Authors\\n\\nCreated and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)\\n\\n## Contributing\\n\\n[Contributing](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CONTRIBUTING.md)\\n\\n## Changelog\\n\\n[Changelog](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/CHANGELOG.md)\\n\\n## License\\n\\n[MIT](https://github.com/BodenmillerGroup/napari-czifile2/blob/main/LICENSE.md)\\n\\n\\n',\n",
       " 'napari-deepmeta': '# napari-deepmeta\\n\\n[![License](https://img.shields.io/github/license/EdgarLefevre/napari-deepmeta?label=license)](https://github.com/EdgarLefevre/napari-deepmeta/blob/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-deepmeta.svg?color=green)](https://pypi.org/project/napari-deepmeta)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-deepmeta.svg?color=green)](https://python.org)\\n[![tests](https://github.com/EdgarLefevre/napari-deepmeta/workflows/tests/badge.svg)](https://github.com/EdgarLefevre/napari-deepmeta/actions)\\n[![codecov](https://codecov.io/gh/EdgarLefevre/napari-deepmeta/branch/main/graph/badge.svg?token=H41ZaCAg31)](https://codecov.io/gh/EdgarLefevre/napari-deepmeta)\\n\\nSegment mouse lungs and metastasis on MRI images.\\n\\nThis plugin is a demo for the [Deepmeta project](https://github.com/EdgarLefevre/DeepMeta).\\n\\n![Lungs segmentation](https://github.com/EdgarLefevre/napari-deepmeta/blob/main/docs/_static/screen_napari_lungs.png?raw=true)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-deepmeta` via [pip]:\\n\\n    pip install napari-deepmeta\\n\\n> We advise you to create a specific python virtual environment in order to have a clean installation.\\n\\n## Usage\\nIn a terminal, just type `napari` to open napari.\\n\\nOpen a (x, 128, 128) image, go in the plugin menu, add deepmeta plugin to the dock viewer and click on the button.\\n\\n![Menu](https://github.com/EdgarLefevre/napari-deepmeta/blob/main/docs/_static/plugin_menu.png?raw=true)\\n\\nBy adding the dock widget, a menu will be created on the left of your Napari instance.\\n\\n![Deepmeta panel](https://github.com/EdgarLefevre/napari-deepmeta/raw/main/docs/_static/panel.png?raw=true)\\n\\nIn this panel you will find two buttons and one checkbox:\\n\\n+ The first button *Run lung seg* process segmentation and show the result (With the widget segment metas, the button is called *Run meta seg*).\\n+ The second, *Reprocess Volume*, is useful when you modify contours. It will reprocess all slices to give you a new volume.\\n+ The checkbox is here to enhance contrast if your image is dark.\\n\\n## Demo\\n\\nIf you just want to see what we\\'ve done, you can try the plugin with the Demo button, this button will load an image and process it\\nas if you use the plugin in a classic way.\\n\\n## Conf file and custom models\\n\\nThe first time you run the plugin a config file will be created at `~/.config/deepmeta/config.ini`.\\n\\n![conf.ini](https://github.com/EdgarLefevre/napari-deepmeta/blob/main/docs/_static/confini.png?raw=true)\\n\\nIn this file you can find parameters for postprocessing loop and the path for the models.\\nFeel free to change values and colors to fit to your needs.\\n\\n>If you want to try another model, you can change the path. Be careful to not having custom objects in you model, otherwise, you\\'ll have to modify the code.\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"napari-deepmeta\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/EdgarLefevre/napari-deepmeta/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n\\n',\n",
       " 'napari-dexp': '# napari-DEXP\\n\\n[![License](https://img.shields.io/pypi/l/napari-dexp.svg?color=green)](https://github.com/royerlab/napari-dexp/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dexp.svg?color=green)](https://pypi.org/project/napari-dexp)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dexp.svg?color=green)](https://python.org)\\n[![tests](https://github.com/royerlab/napari-dexp/workflows/tests/badge.svg)](https://github.com/royerlab/napari-dexp/actions)\\n[![codecov](https://codecov.io/gh/royerlab/napari-dexp/branch/master/graph/badge.svg)](https://codecov.io/gh/royerlab/napari-dexp)\\n\\nA plugin to interface [DEXP](https://github.com/royerlab/dexp) with [napari](https://github.com/napari/napari).\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-dexp` via [pip]:\\n\\n    pip install napari-dexp\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-dexp\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/royerlab/napari-dexp/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-dv': '# napari-dv\\n\\n[![License](https://img.shields.io/pypi/l/napari-dv.svg?color=green)](https://github.com/tlambert03/napari-dv/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dv.svg?color=green)](https://pypi.org/project/napari-dv)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dv.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-dv/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-dv/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-dv/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-dv)\\n\\nDeltavision/MRC file reader for napari\\n\\n## Installation\\n\\nYou can install `napari-dv` via [pip]:\\n\\n    pip install napari-dv\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"napari-dv\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[file an issue]: https://github.com/tlambert03/napari-dv/issues\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n\\n\\n',\n",
       " 'napari-dvid': '# napari-dvid\\n\\n[![License](https://img.shields.io/pypi/l/napari-dvid.svg?color=green)](https://github.com/emmazhou/napari-dvid/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dvid.svg?color=green)](https://pypi.org/project/napari-dvid)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dvid.svg?color=green)](https://python.org)\\n[![tests](https://github.com/emmazhou/napari-dvid/workflows/tests/badge.svg)](https://github.com/emmazhou/napari-dvid/actions)\\n[![codecov](https://codecov.io/gh/emmazhou/napari-dvid/branch/master/graph/badge.svg)](https://codecov.io/gh/emmazhou/napari-dvid)\\n\\nDVID loader for napari, from a url\\n\\n---\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-dvid` via [pip]:\\n\\n    pip install napari-dvid\\n\\n## Examples\\n\\nOnce installed, run `napari --with napari-dvid` to get the plugin sidebar:\\n\\n![Screenshot](screenshot.png)\\n\\nPaste in a URL to a DVID volume and hit \"Load\" to load the volume! As an example, try:\\n\\n`https://emdata.janelia.org/api/node/ab6e610d4/grayscale/raw/0_1_2/256_256_256/7500_7000_4400`\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [MIT] license,\\n\"napari-dvid\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[mit]: http://opensource.org/licenses/MIT\\n[bsd-3]: http://opensource.org/licenses/BSD-3-Clause\\n[gnu gpl v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[gnu lgpl v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[apache software license 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[mozilla public license 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/emmazhou/napari-dvid/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[pypi]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-dzi-zarr': '# napari-dzi-zarr\\n\\n[![License](https://img.shields.io/pypi/l/napari-dzi-zarr.svg?color=green)](https://github.com/napari/napari-dzi-zarr/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-dzi-zarr.svg?color=green)](https://pypi.org/project/napari-dzi-zarr)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-dzi-zarr.svg?color=green)](https://python.org)\\n[![tests](https://github.com/manzt/napari-dzi-zarr/workflows/tests/badge.svg)](https://github.com/manzt/napari-dzi-zarr/actions)\\n\\nAn experimental plugin for viewing Deep Zoom Images (DZI) with napari + zarr + dask.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n## Description \\n\\nThe [DZI File Format](https://github.com/openseadragon/openseadragon/wiki/The-DZI-File-Format) \\nis a pyramidal tile source specification where individual tiles are RGB/RGBA JPEG/PNG images. \\nDZI is a very popular tile source for zoomable web-viewers like \\n[OpenSeadragon](https://openseadragon.github.io/), and as a result many tile sources are available over \\nHTTP. This plugin wraps a DZI tile source (local or remote) as a multiscale Zarr, where each pyramidal level is a `zarr.Array` of shape `(level_height, level_width, 3/4)`, allowing the same images to be viewed \\nin `napari` + `dask`.\\n\\n## Installation\\n\\nYou can install `napari-dzi-zarr` via [pip]:\\n\\n    pip install napari-dzi-zarr\\n\\n\\n## Usage\\n\\nThis high-resolution scan of Rembrandt\\'s Night Watch is available thanks to [R.G Erdmann](https://twitter.com/erdmann). More examples can be found on [boschproject.org](http://boschproject.org).\\n\\n    $ napari http://hyper-resolution.org/dzi/Rijksmuseum/SK-C-5/SK-C-5_VIS_20-um_2019-12-21.dzi\\n\\n![Rembrandt\\'s Night Watch in napari](./night_watch_napari.png)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox].\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-dzi-zarr\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/manzt/napari-dzi-zarr/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-em-reader': '# napari-em-reader\\n\\n[![License](https://img.shields.io/pypi/l/napari-em-reader.svg?color=green)](https://github.com/brisvag/napari-em-reader/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-em-reader.svg?color=green)](https://pypi.org/project/napari-em-reader)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-em-reader.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-em-reader/workflows/tests/badge.svg)](https://github.com/brisvag/napari-em-reader/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-em-reader/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-em-reader)\\n\\nA napari plugin to read .em files\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-em-reader` via [pip]:\\n\\n    pip install napari-em-reader\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-em-reader\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/brisvag/napari-em-reader/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-features': '',\n",
       " 'napari-hdf5-labels-io': '# Description\\n\\nThis IO plugin lets you to store your progress in a single file (.h5 extension). It stores not only the layer\\'s data but also its metadata, meaning that in some way, this IO can be seen as a project file generator.\\n\\nThe current supported layer types are: images, labels and dots.\\n\\nThis plugin was developed to create a connection between napari (for labeling) and YAPiC (a deep learning segmentation tool).\\n\\n# Who is this for?\\n\\nThis plugin is meant to be used for any napari user wanting to store their progress in a single file and for those which use napari as a labeling tool.\\n\\nIt supports any data dimensionality and it was designed to improve the memory efficiency when storing label layers.\\n\\nAdditionally, YAPiC supports the files generated by this IO to perform image segmentation.\\n\\n# Quick start\\n\\n## Saving .h5 files\\n\\nWith `napari-hdf5-labels-io installed`, use napari as alway. once you are done, click in File, click in Save Selected Layer(s)... (Ctrl+S) or Save All Layers... (Ctrl+Shift+S) and write the output file name as `filename.h5`. Including the \".h5\" extension at the end of the name will automatically activate the plugin.\\n\\n## Opening .h5 files\\n\\nTo open a .h5 file written with this plugin, you can open this file as any other (either by the Open File option in the File menu or dragging it to the main window).',\n",
       " 'napari-imc': '# napari-imc\\n\\n<a href=\"https://pypi.org/project/napari-imc/\">\\n    <img src=\"https://img.shields.io/pypi/v/napari-imc\" alt=\"PyPI\" />\\n</a>\\n<a href=\"https://github.com/BodenmillerGroup/napari-imc/blob/main/LICENSE.md\">\\n    <img src=\"https://img.shields.io/pypi/l/napari-imc\" alt=\"License\" />\\n</a>\\n<a href=\"https://www.python.org/\">\\n    <img src=\"https://img.shields.io/pypi/pyversions/napari-imc\" alt=\"Python\" />\\n</a>\\n<a href=\"https://github.com/BodenmillerGroup/napari-imc/issues\">\\n    <img src=\"https://img.shields.io/github/issues/BodenmillerGroup/napari-imc\" alt=\"Issues\" />\\n</a>\\n<a href=\"https://github.com/BodenmillerGroup/napari-imc/pulls\">\\n    <img src=\"https://img.shields.io/github/issues-pr/BodenmillerGroup/napari-imc\" alt=\"Pull requests\" />\\n</a>\\n\\nImaging Mass Cytometry (IMC) file type support for napari\\n\\nOpen .mcd/.txt files and interactively view panoramas and multi-channel acquisitions co-registered in the machine\\'s coordinate system using napari\\n\\n## Installation\\n\\nYou can install napari-imc via [pip](https://pypi.org/project/pip/):\\n\\n    pip install napari-imc\\n\\n## Authors\\n\\nCreated and maintained by Jonas Windhager [jonas.windhager@uzh.ch](mailto:jonas.windhager@uzh.ch)\\n\\n## Contributing\\n\\n[Contributing](https://github.com/BodenmillerGroup/napari-imc/blob/main/CONTRIBUTING.md)\\n\\n## Changelog\\n\\n[Changelog](https://github.com/BodenmillerGroup/napari-imc/blob/main/CHANGELOG.md)\\n\\n## License\\n\\n[MIT](https://github.com/BodenmillerGroup/napari-imc/blob/main/LICENSE.md)\\n\\n\\n',\n",
       " 'napari-itk-io': '# napari-itk-io\\n\\n[![License](https://img.shields.io/pypi/l/napari-itk-io.svg?color=green)](https://github.com/InsightSoftwareConsortium/napari-itk-io/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-itk-io.svg?color=green)](https://pypi.org/project/napari-itk-io)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-itk-io.svg?color=green)](https://python.org)\\n[![tests](https://github.com/InsightSoftwareConsortium/napari-itk-io/workflows/tests/badge.svg)](https://github.com/InsightSoftwareConsortium/napari-itk-io/actions)\\n[![codecov](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io/branch/master/graph/badge.svg)](https://codecov.io/gh/InsightSoftwareConsortium/napari-itk-io)\\n\\nFile IO with itk for napari.\\n\\nSupported image file formats:\\n\\n- [BioRad](http://www.bio-rad.com/)\\n- [BMP](https://en.wikipedia.org/wiki/BMP_file_format)\\n- [DICOM](http://dicom.nema.org/)\\n- [DICOM Series](http://dicom.nema.org/)\\n- [ITK HDF5](https://support.hdfgroup.org/HDF5/)\\n- [JPEG](https://en.wikipedia.org/wiki/JPEG_File_Interchange_Format)\\n- [GE4,GE5,GEAdw](http://www3.gehealthcare.com)\\n- [Gipl (Guys Image Processing Lab)](https://www.ncbi.nlm.nih.gov/pubmed/12956259)\\n- [LSM](http://www.openwetware.org/wiki/Dissecting_LSM_files)\\n- [MetaImage](https://itk.org/Wiki/ITK/MetaIO/Documentation)\\n- [MINC 2.0](https://en.wikibooks.org/wiki/MINC/SoftwareDevelopment/MINC2.0_File_Format_Reference)\\n- [MGH](https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat)\\n- [MRC](http://www.ccpem.ac.uk/mrc_format/mrc_format.php)\\n- [NifTi](https://nifti.nimh.nih.gov/nifti-1)\\n- [NRRD](http://teem.sourceforge.net/nrrd/format.html)\\n- [Portable Network Graphics (PNG)](https://en.wikipedia.org/wiki/Portable_Network_Graphics)\\n- [Tagged Image File Format (TIFF)](https://en.wikipedia.org/wiki/TIFF)\\n- [VTK legacy file format for images](http://www.vtk.org/VTK/img/file-formats.pdf)\\n\\nFor DICOM Series, select the folder containing the series with *File -> Open\\nFolder...*. The first series will be selected and sorted spatially.\\n\\n## Installation\\n\\nYou can install `napari-itk-io` via [pip]:\\n\\n    pip install napari-itk-io\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\nFollow the [itk contributing\\nguidelines](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CONTRIBUTING.md)\\nand the [itk code of\\nconduct](https://github.com/InsightSoftwareConsortium/ITK/blob/master/CODE_OF_CONDUCT.md).\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\"napari-itk-io\" is free and open source software.\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/InsightSoftwareConsortium/napari-itk-io/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-lazy-openslide': '# napari-lazy-openslide\\n\\n[![License](https://img.shields.io/pypi/l/napari-lazy-openslide.svg?color=green)](https://github.com/manzt/napari-lazy-openslide/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-lazy-openslide.svg?color=green)](https://pypi.org/project/napari-lazy-openslide)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-lazy-openslide.svg?color=green)](https://python.org)\\n[![tests](https://github.com/manzt/napari-lazy-openslide/workflows/tests/badge.svg)](https://github.com/manzt/napari-lazy-openslide/actions)\\n\\nAn experimental plugin to lazily load multiscale whole-slide tiff images with openslide and dask.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\n**Step 1.)** Make sure you have OpenSlide installed. Download instructions [here](https://openslide.org/download/).\\n\\n> NOTE: Installation on macOS is easiest via Homebrew: `brew install openslide`. Up-to-date and multiplatform \\n> binaries for `openslide` are also avaiable via `conda`: `conda install -c sdvillal openslide-python`\\n\\n**Step 2.)** Install `napari-lazy-openslide` via [pip]:\\n\\n    pip install napari-lazy-openslide\\n\\n## Usage\\n\\nThis plugin tries to be conservative with what files it will attempt to provide a reader.\\nIt will only attempt to read `.tif` and `.tiff` files that `openslide` will open and are \\ndetected as multiscale (`openslide.OpenSlide.level_count > 1`). Under the hood, \\n`napari-lazy-openslide` wraps the `openslide` reader with a valid `zarr.Store` where each \\neach pyramidal level is exposed as a separate `zarr.Array` with shape `(y,x,4)`.\\n\\nThe plugin is experimental and has only been tested with `CAMELYON16` and `CAMELYON17` datasets, \\nwhich can be downloaded [here](https://camelyon17.grand-challenge.org/Data/).\\n\\n```bash\\n$ napari tumor_004.tif\\n```\\n\\n![Interactive deep zoom of whole-slide image](tumor_004.gif)\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-lazy-openslide\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/manzt/napari-lazy-openslide/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-manual-split-and-merge-labels': '# napari-manual-split-and-merge-labels\\n\\n[![License](https://img.shields.io/pypi/l/napari-manual-split-and-merge-labels.svg?color=green)](https://github.com/haesleinhuepf/napari-manual-split-and-merge-labels/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-manual-split-and-merge-labels.svg?color=green)](https://pypi.org/project/napari-manual-split-and-merge-labels)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-manual-split-and-merge-labels.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-manual-split-and-merge-labels/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-manual-split-and-merge-labels/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-manual-split-and-merge-labels/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-manual-split-and-merge-labels)\\n\\nSplit and merge labels in napari manually\\n\\n![](https://github.com/haesleinhuepf/napari-manual-split-and-merge-labels/raw/main/docs/images/split_and_merge_demo.gif)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-manual-split-and-merge-labels` via [pip]:\\n\\n    pip install napari-manual-split-and-merge-labels\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-manual-split-and-merge-labels\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/haesleinhuepf/napari-manual-split-and-merge-labels/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[image.sc]: https://image.sc\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\\n\\n\\n',\n",
       " 'napari-mat-images': \"# napari-mat-images\\n\\n[![PyPI version](https://img.shields.io/pypi/v/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)\\n\\n[![Python versions](https://img.shields.io/pypi/pyversions/napari-mat-images.svg)](https://pypi.org/project/napari-mat-images)\\n\\n[![See Build Status on Azure Pipelines](https://dev.azure.com/hectormz-1/napari-mat-images/_apis/build/status/hectormz.napari-mat-images?branchName=main)](https://dev.azure.com/hectormz-1/napari-mat-images/_build/latest?definitionId=1&branchName=main)\\n\\n## Features\\n\\nThis plugin loads image variables stored in `MATLAB` `.mat` files into [napari](https://github.com/napari/napari).\\n\\nIt loads any variable that looks like an image.\\nPresently, that includes any array with more than two dimensions with size greater than 20 pixels (determined by `shape_is_image()`).\\n\\nIf loading a variable with 3 or more dimensions, the plugin assumes that it is a stack of images, and the dimension with greatest size is the axis of the stack.\\n\\n### Loading Large Files\\n\\nIf loading a large `.mat` file saved in `HDF5`/`v7.3` format, chunks of the images are loaded as needed, resulting in fast initial load, but potentially slower scrolling.\\n\\nSlices of the image stacks are randomly sampled to determine min/max contrast values.\\n\\n## Requirements\\n\\nThis plugin relies on `scipy` to load small `.mat` files and `h5py` (with `dask`) to load larger `HDF5`/`v7.3` `.mat` files.\\n\\nIt implicitly requires `napari` for use.\\n\\n## Installation\\n\\n`napari-mat-images` requires [napari](https://github.com/napari/napari) to be installed, although it is not listed as a requirement for installation.\\nThis plugin relies on plugin functionality found in `napari` version \\\\> `0.2.12`. This can be installed via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):\\n\\n    $ pip install napari>0.2.12\\n\\nYou can install `napari-mat-images` via [pip](https://pypi.org/project/pip/) from [PyPI](https://pypi.org/project):\\n\\n    $ pip install napari-mat-images\\n\\n## Usage\\n\\nOnce installed, the plugin will be used whenever trying to load a `.mat` file.\\nThis can be done from the `napari` GUI or commandline:\\n\\n    $ napari my_file.mat\\n\\n## Contributing\\n\\nContributions are very welcome.\\nTests can be run with [pytest](https://docs.pytest.org/en/latest/),\\nplease ensure the coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3](http://opensource.org/licenses/BSD-3-Clause) license, `napari-mat-images` is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue](https://github.com/hectormz/napari-mat-images/issues) along with a detailed description.\\n\\n---\\n\\nThis [napari](https://github.com/napari/napari) plugin was generated with [Cookiecutter](https://github.com/audreyr/cookiecutter) along with [napari](https://github.com/napari/napari)\\\\'s [cookiecutter-napari-plugin](https://github.com/napari/cookiecutter-napari-plugin) template.\\n\\n\\n\",\n",
       " 'napari-medical-image-formats': '# napari-medical-image-formats\\n\\n[![License](https://img.shields.io/pypi/l/napari-medical-image-formats.svg?color=green)](https://github.com/MBPhys/napari-medical-image-formats/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-medical-image-formats.svg?color=green)](https://pypi.org/project/napari-medical-image-formats)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-medical-image-formats.svg?color=green)](https://python.org)\\n\\n\\nA Plugin in order to read medical image formats such as DICOM and NIfTI without any meta informations. This will be updated soon.\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `napari-medical-image-formats` via [pip]:\\n\\n    pip install napari-medical-image-formats\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-medical-image-formats\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/MBPhys/napari-medical-image-formats/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-micromanager': '# napari-micromanager\\n\\n[![License](https://img.shields.io/pypi/l/napari-micromanager.svg?color=green)](https://github.com/napari/napari-micromanager/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-micromanager.svg?color=green)](https://pypi.org/project/napari-micromanager)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-micromanager.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-micromanager/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-micromanager/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-micromanager/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-micromanager)\\n\\nGUI interface between napari and micromanager\\n\\n🚧 Experimental!  Work in progress!  Here be 🐲 🚧\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-micromanager` via [pip]:\\n\\n    pip install napari-micromanager\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n### Launching napari with plugin\\nYou can launch napari and automatically load this plugin using the `launch-dev.py` script:\\n\\n```bash\\npython launch-dev.py\\n```\\n\\nAlternatively you can run:\\n\\n```bash\\nnapari -w micromanager\\n```\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-micromanager\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/tlambert03/napari-micromanager/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-mri': '# napari-mri\\n\\n[![License](https://img.shields.io/pypi/l/napari-mri.svg?color=green)](https://github.com/sahas111/napari-mri/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-mri.svg?color=green)](https://pypi.org/project/napari-mri)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-mri.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sahas111/napari-mri/workflows/tests/badge.svg)](https://github.com/sahas111/napari-mri/actions)\\n[![codecov](https://codecov.io/gh/sahas111/napari-mri/branch/master/graph/badge.svg)](https://codecov.io/gh/sahas111/napari-mri)\\n\\nA simple plugin to use with napari for 3D-viewing of Magnetic Resonance Imaging file formats\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-mri` via [pip]:\\n\\n    pip install napari-mri\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-mri\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sahas111/napari-mri/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-ndtiffs': '# napari-ndtiffs\\n\\n[![License](https://img.shields.io/pypi/l/napari-ndtiffs.svg?color=green)](https://raw.githubusercontent.com/tlambert03/napari-ndtiffs/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-ndtiffs.svg?color=green)](https://pypi.org/project/napari-ndtiffs)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-ndtiffs.svg?color=green)](https://python.org)\\n[![tests](https://github.com/tlambert03/napari-ndtiffs/workflows/tests/badge.svg)](https://github.com/tlambert03/napari-ndtiffs/actions)\\n[![codecov](https://codecov.io/gh/tlambert03/napari-ndtiffs/branch/master/graph/badge.svg)](https://codecov.io/gh/tlambert03/napari-ndtiffs)\\n\\nnapari plugin for nd tiff folders with optional OpenCl-based deskewing.\\n\\nBuilt-in support for folders of (skewed) lattice light sheet tiffs.\\n\\n![napari-ndtiffs demo](https://github.com/tlambert03/napari-ndtiffs/raw/master/demo.gif)\\n\\n----------------------------------\\n\\n*This [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.*\\n\\n## Features\\n\\n- Drag and drop a folder of tiffs onto napari window to view easily \\n  - (currently designed to detect  lattice light sheet tiffs, but easily\\n    adjustable)\\n- If lattice `Settings.txt` file is found, will deskew automatically (only if\\n  necessary)\\n- Lazily loads dataset on demand.  quickly load preview your data.\\n- Handles `.zip` archives as well!  Just directly compress your tiff folder,\\n  then drop it into napari.\\n- All-openCL deskewing, works on GPU as well as CPU, falls back to scipy if\\n  pyopencl is unavailable.\\n\\nIt would not be hard to support arbitrary filenaming patterns!  If you have a\\nfolder of tiffs with a consistent naming scheme and would like to take advantage\\nof this plugin, feel free to open an issue!\\n\\n## Installation\\n\\nYou can install `napari-ndtiffs` via [pip]:\\n\\n```shell\\npip install napari-ndtiffs\\n```\\n\\nTo also install PyOpenCL (for faster deskewing):\\n\\n```shell\\npip install napari-ndtiffs[opencl]\\n```\\n\\n## Usage\\n\\nIn most cases, just drop your folder onto napari, or use `viewer.open(\"path\")`\\n\\n### Overriding parameters\\n\\nYou can control things like voxel size and deskewing angle as follows:\\n\\n```python\\nfrom napari_ndtiffs import parameter_override\\nimport napari\\n\\nviewer = napari.Viewer()\\nwith parameter_override(angle=45, name=\"my image\"):\\n    viewer.open(\"path/to/folder\", plugin=\"ndtiffs\")\\n```\\n\\nValid keys for `parameter_override` include:\\n\\n- **dx**: (`float`) the pixel size, in microns\\n- **dz**: (`float`)the z step size, in microns\\n- **deskew**: (`bool`) whether or not to deskew, (by default, will deskew if angle > 0, or if a lattice metadata file is detected that requires deskewing) \\n- **angle**: (`float`) the angle of the light sheet relative to the coverslip\\n- **padval**: (`float`) the value with which to pad the image edges when deskewing (default is 0)\\n- **contrast_limits**: (`2-tuple of int`) (min, max) contrast_limits to use when viewing the image\\n- **name**: (`str`) an optional name for the image\\n\\n### Sample data\\n\\nTry it out with test data: [download sample data](https://www.dropbox.com/s/up4ywrn2sckjunc/lls_mitosis.zip?dl=1)\\n\\nYou can unzip if you like, or just drag the zip file onto the napari window.\\n\\nOr, from command line, use:\\n\\n```bash\\nnapari path/to/lls_mitosis.zip\\n```\\n\\n## Debugging\\n\\nTo monitor file io and deskew activity, enter the following in the napari console:\\n\\n```python\\nimport logging\\nlogging.getLogger(\\'napari_llsfolder\\').setLevel(\\'DEBUG\\')\\n```\\n\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-ndtiffs\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/tlambert03/napari-ndtiffs/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-nikon-nd2': '# napari-nikon-nd2\\n\\n[![License](https://img.shields.io/pypi/l/napari-nikon-nd2.svg?color=green)](https://github.com/cwood1967/napari-nikon-nd2/blob/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-nikon-nd2.svg?color=green)](https://pypi.org/project/napari-nikon-nd2)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-nikon-nd2.svg?color=green)](https://python.org)\\n[![tests](https://github.com/cwood1967/napari-nikon-nd2/workflows/tests/badge.svg)](https://github.com/cwood1967/napari-nikon-nd2/actions)\\n[![codecov](https://codecov.io/gh/cwood1967/napari-nikon-nd2/branch/main/graph/badge.svg)](https://codecov.io/gh/cwood1967/napari-nikon-nd2)\\n\\nOpens Nikon ND2 files into napari. This plugin uses the [nd2reader] and [pims] python packages. \\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-nikon-nd2` via [pip]:\\n\\n    pip install napari-nikon-nd2\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [Apache Software License 2.0] license,\\n\"napari-nikon-nd2\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n## Credits\\n\\nThis [napari] plugin was created using [Napari Delta Vision Reader] and\\nthe [Allen Institute IO] plugin as examples.\\n\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/cwood1967/napari-nikon-nd2/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[nd2reader]: https://github.com/rbnvrw/nd2reader\\n[pims]: https://github.com/soft-matter/pims\\n[Allen Institute IO]: https://github.com/AllenCellModeling/napari-aicsimageio\\n[Napari Delta Vision Reader]: https://github.com/tlambert03/napari-dv\\n\\n',\n",
       " 'napari-oclrfc': '# napari-oclrfc\\n\\n[![License](https://img.shields.io/pypi/l/napari-oclrfc.svg?color=green)](https://github.com/haesleinhuepf/napari-oclrfc/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-oclrfc.svg?color=green)](https://pypi.org/project/napari-oclrfc)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-oclrfc.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-oclrfc/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-oclrfc/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-oclrfc/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-oclrfc)\\n\\n[py-clEsperanto](https://github.com/clEsperanto/pyclesperanto_prototype) meets [scikit-learn](https://scikit-learn.org/stable/)\\n\\nA yet experimental OpenCL-based Random Forest Classifier for pixel and labeled object classification in [napari].\\n\\n![](https://github.com/haesleinhuepf/napari-oclrfc/raw/master/images/screenshot.png)\\nThe processed example image [maize_clsm.tif](https://github.com/dlegland/mathematical_morphology_with_MorphoLibJ/blob/master/sampleImages/maize_clsm.tif)\\nis licensed by David Legland under \\n[CC-BY 4.0 license](https://github.com/dlegland/mathematical_morphology_with_MorphoLibJ/blob/master/LICENSE)\\n\\nFor using OpenCL-based Random Forest Classifiers for pixel classification in python, check out [oclrfc](https://github.com/haesleinhuepf/oclrfc).\\n\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-oclrfc` via [pip]. Note: you also need [pyopencl](https://documen.tician.de/pyopencl/).\\n\\n    conda install pyopencl\\n    pip install napari-oclrfc\\n    \\nIn case of issues in napari, make sure these dependencies are installed properly:\\n    \\n    pip install pyclesperanto_prototype\\n    pip install oclrfc\\n\\n## Usage\\n\\nOpen an image in napari and add a labels layer. Annotate foreground and background with two different label identifiers. You can also add a third, e.g. a membrane-like region in between to improve segmentation quality.\\n![img.png](https://github.com/haesleinhuepf/napari-oclrfc/raw/master/images/img.png)\\n\\nClick the menu `Plugins > OpenCL Random Forest Classifiers > Train pixel classifier`. \\nConsider changing the `featureset`. There are three options for selecting \\nsmall (about 1 pixel sized) objects, \\nmedium (about 5 pixel sized) object and \\nlarge (about 25 pixel sized) objects.\\nMake sure the right image and annotation layers are selected and click on `Run`.\\n\\n![img_1.png](https://github.com/haesleinhuepf/napari-oclrfc/raw/master/images/img_1.png)\\n\\nThe classifier was saved as `temp.cl` to disc. You can later re-use it by clicking the menu `Plugins > OpenCL Random Forest Classifiers > Predict pixel classifier`\\n\\nOptional: Hide the annotation layer.\\n\\nClick the menu `Plugins > OpenCL Random Forest Classifiers > Connected Component Labeling`.\\nMake sure the right labels layer is selected. It is supposed to be the result layer from the pixel classification.\\nSelect the `object class identifier` you used for annotating objects, that\\'s the intensity you drew on objects in the annotation layer.\\nHint: If you want to analyse touching neigbors afterwards, activate the `fill gaps between labels` checkbox.\\nClick on the `Run` button.\\n![img_2.png](https://github.com/haesleinhuepf/napari-oclrfc/raw/master/images/img_2.png)\\n\\nOptional: Hide the pixel classification result layer. Change the opacity of the connected component labels layer.\\n\\nAdd a new labels layer and annotate different object classes by drawing lines through them. \\nIn the following example objects with different size and shape were annotated in three classes:\\n* round, small\\n* round, large\\n* elongated\\n![img_3.png](https://github.com/haesleinhuepf/napari-oclrfc/raw/master/images/img_3.png)\\n  \\nClick the menu `Plugins > OpenCL Random Forest Classifiers > Train label classifier`. Select the right layers for training.\\nThe labels layer should be the result from connected components labeling.\\nThe annotation layer should be the just annotated object classes layer.\\nSelect the right features for training. Click on the `Run` button. \\nAfter training, the classifier will be stored to disc in the file you specified.\\nYou can later re-use it by clicking the menu `Plugins > OpenCL Random Forest Classifiers > Predict label classifier`\\n\\n![img_5.png](https://github.com/haesleinhuepf/napari-oclrfc/raw/master/images/img_5.png)\\n\\nThis is an experimental napari plugin. Feedback is very welcome!\\n\\n## Contributing\\n \\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-oclrfc\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [open a thread on image.sc](https://image.sc) along with a detailed description and tag [@haesleinhuepf](https://github.com/haesleinhuepf).\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/haesleinhuepf/napari-oclrfc/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-ome-zarr': '# Description\\n\\nThis plugin provides a reader for zarr backed OME-NGFF images in napari. The reader\\nwill inspect the `.zattrs` metadata provided and pass any relevant metadata, including channel, scale and colormap metadata. \\n\\n![Opening an ome-zarr image in napari](https://i.imgur.com/tf9IqRA.gif)\\n\\nThe example above uses the image at http://idr.openmicroscopy.org/webclient/?show=image-6001240\\n\\n# Supported Data\\n\\nThis plugin is designed to allow bioimaging researchers and analysts to explore their\\nmulti-resolution images stored in Zarr filesets (according to the [OME zarr spec](https://ngff.openmicroscopy.org/latest/))\\nwithout needing an intricate understanding of zarr, or the spec itself.\\n\\nThis plugin supports reading all images recognised as ome-zarr, namely, containing\\nwell-formed `.zattrs` and `.zgroup` files, as well as the appropriate directory \\nhierarchy as described in the [spec](https://ngff.openmicroscopy.org/latest/). \\nThe image metadata from OMERO will be used to set channel names, colormaps and rendering settings in napari.\\n\\n# Quickstart\\n\\nYou can open local or remote images using `napari` at the terminal and the path to your file:\\n\\n```\\n$ napari \\'https://s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/\\'\\n\\n# also works with local files\\n$ napari 6001240.zarr\\n```\\n\\nOR in python:\\n\\n```python\\nimport napari\\n\\nviewer = napari.Viewer()\\nviewer.open(\\'https://s3.embassy.ebi.ac.uk/idr/zarr/v0.1/6001240.zarr/\\')\\nnapari.run()\\n```\\nIf a single zarray is passed to the plugin, it will be opened without the use of\\nthe metadata:\\n\\n```\\n$ napari \\'/tmp/6001240.zarr/0\\'\\n```\\n\\nIf an image group contains labels, they will also be opened, and added as a \\nseparate layer in napari.\\n\\nWhen the labels group metadata additionally contains `\"rgba\"` and `\"properties\"` keys, \\nthe labels will be given appropriate colors and the properties will be displayed \\nin the status bar.\\n\\nWorking with ome-zarr images can be more convenient using the command-line interface\\nand utility functions of our associated library `ome-zarr`. For more information \\nplease see the [package documentation](https://pypi.org/project/ome-zarr/) for `ome-zarr`.\\n\\n# Getting Help\\n\\nIf you discover a bug with the plugin, or would like to request a new feature, please\\nraise an issue on our repository at https://github.com/ome/ome-zarr-py.\\n\\nIf you would like assistance with using the plugin, or converting images to\\nome-zarr format, please reach out on [image.sc](https://forum.image.sc/).\\n\\n# How to Cite\\n\\n## To cite OME-NGFF:\\n\\n[Next-generation file format (NGFF) specifications for storing bioimaging data in the cloud](https://ngff.openmicroscopy.org/0.1/). J. Moore, et al. Editors. Open Microscopy Environment Consortium, 20 November 2020. This edition of the specification is https://ngff.openmicroscopy.org/0.1/. The latest edition is available at https://ngff.openmicroscopy.org/latest/. ([doi:10.5281/zenodo.4282107](https://doi.org/10.5281/zenodo.4282107))\\n\\n## To cite this plugin:\\n\\n[ome-zarr-py: Experimental implementation of next-generation file format (NGFF) specifications for storing bioimaging data in the cloud.](https://doi.org/10.5281/zenodo.4113931) OME; et al. 06 October 2020. URL: https://doi.org/10.5281/zenodo.4113931',\n",
       " 'napari-omero': '# napari-omero\\n\\n[![License](https://img.shields.io/github/license/tlambert03/napari-omero)](LICENSE)\\n[![Version](https://img.shields.io/pypi/v/napari-omero.svg)](https://pypi.python.org/pypi/napari-omero)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-omero.svg)](https://python.org)\\n[![CI](https://github.com/tlambert03/napari-omero/workflows/CI/badge.svg)](https://github.com/tlambert03/napari-omero/actions)\\n<!-- [![conda-forge](https://img.shields.io/conda/vn/conda-forge/napari-omero)](https://anaconda.org/conda-forge/napari-omero) -->\\n\\nThis package provides interoperability between the\\n[OMERO](https://www.openmicroscopy.org/omero/) image management platform, and\\n[napari](https://github.com/napari/napari): a fast, multi-dimensional image\\nviewer for python.\\n\\nIt provides a GUI interface for browsing an OMERO instance from within napari,\\nas well as command line interface extensions for both OMERO and napari CLIs.\\n\\n![demo](https://github.com/tlambert03/napari-omero/blob/master/demo.gif?raw=true)\\n\\n## Features\\n\\n- GUI interface to browse remote OMERO data, with thumbnail previews.\\n- Loads remote nD images from an OMERO server into napari\\n- Planes are loading on demand as sliders are moved (\"lazy loading\").\\n- session management (login memory)\\n- OMERO rendering settings (contrast limits, colormaps, active channels, current\\n  Z/T position) are applied in napari\\n\\n### as a napari dock widget\\n\\nTo launch napari with the OMERO browser added, [install](#installation) this\\npackage and run:\\n\\n```bash\\nnapari_omero\\n```\\n\\nThe OMERO browser widget can also be manually added to the napari viewer:\\n\\n```python\\nimport napari\\nfrom napari_omero import OMEROWidget\\n\\nwith napari.gui_qt():\\n    viewer = napari.Viewer()\\n    viewer.window.add_dock_widget(OMEROWidget(), area=\"right\")\\n```\\n\\n### as a napari plugin\\n\\nThis package provides a napari reader plugin that accepts OMERO resources as\\n\"proxy strings\" (e.g. `Image:<ID>`) or as [OMERO webclient\\nURLS](https://help.openmicroscopy.org/urls-to-data.html).\\n\\n```python\\nviewer = napari.Viewer()\\n\\n# omero object identifier string\\nviewer.open(\"Image:1\", plugin=\"omero\")\\n\\n# or URLS: https://help.openmicroscopy.org/urls-to-data.html\\nviewer.open(\"http://yourdomain.example.org/omero/webclient/?show=image-314\")\\n```\\n\\nthese will also work on the napari command line interface, e.g.:\\n\\n```bash\\nnapari Image:1\\n# or\\nnapari http://yourdomain.example.org/omero/webclient/?show=image-314\\n```\\n\\n### as an OMERO CLI plugin\\n\\nThis package also serves as a plugin to the OMERO CLI\\n\\n```bash\\nomero napari view Image:1\\n```\\n\\n- ROIs created in napari can be saved back to OMERO via a \"Save ROIs\" button.\\n- napari viewer console has BlitzGateway \\'conn\\' and \\'omero_image\\' in context.\\n\\n## installation\\n\\nRequires python 3.7 - 3.9.\\n\\nIt\\'s easiest to install `omero-py` from conda, so the recommended install\\nprocedure is to first create a new conda environment (here called \"`omero`\")\\nwith `omero-py` installed from the `ome` channel, and then use `pip` to\\ninstall `napari-omero` (until we have a conda package available).\\n\\n```sh\\nconda create -n omero -c ome python=3.7 omero-py\\nconda activate omero\\npip install napari-omero\\n```\\n\\n## issues\\n\\n| ❗  | This is alpha software & some things will be broken or sub-optimal!  |\\n| --- | -------------------------------------------------------------------- |\\n\\n- experimental & definitely still buggy!  [Bug\\n  reports](https://github.com/tlambert03/napari-omero/issues/new) are welcome!\\n- remote loading can be very slow still... though this is not strictly an issue\\n  of this plugin.  Datasets are wrapped as delayed dask stacks, and remote data\\n  fetching time can be significant.  Plans for [asynchronous\\n  rendering](https://napari.org/docs/explanations/rendering.html) in napari and\\n  [tiled loading from OMERO](https://github.com/tlambert03/napari-omero/pull/1)\\n  may eventually improve the subjective performance... but remote data loading\\n  will likely always be a limitation here.\\n\\n## contributing\\n\\nContributions are welcome!  To get setup with a development environment:\\n\\n```bash\\n# clone this repo:\\ngit clone https://github.com/tlambert03/napari-omero.git\\n# change into the new directory\\ncd napari-omero\\n# create conda environment\\nconda env create -f environment.yml\\n# activate the new env\\nconda activate napari-omero\\n```\\n\\nTo maintain good code quality, this repo uses\\n[flake8](https://gitlab.com/pycqa/flake8),\\n[mypy](https://github.com/python/mypy), and\\n[black](https://github.com/psf/black).  To enforce code quality when you commit\\ncode, you can install pre-commit\\n\\n```bash\\n# install pre-commit which will run code checks prior to commits\\npre-commit install\\n```\\n\\nThe original OMERO data loader and CLI extension was created by [Will\\nMoore](https://github.com/will-moore).\\n\\nThe napari reader plugin and GUI browser was created by [Talley\\nLambert](https://github.com/tlambert03/)\\n\\n\\n',\n",
       " 'napari-plot-profile': '# napari-plot-profile\\n\\n[![License](https://img.shields.io/pypi/l/napari-plot-profile.svg?color=green)](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-plot-profile.svg?color=green)](https://pypi.org/project/napari-plot-profile)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-plot-profile.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-plot-profile/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-plot-profile/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-plot-profile/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-plot-profile)\\n\\nPlot intensities along a line in [napari].\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/napari-plot-profile-screencast.gif)\\n\\n## Usage\\n\\n* Open some images in [napari].\\n  \\n* Add a shapes layer.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/add_shapes_layer_screenshot.png)\\n  \\n* Activate the line drawing tool or the path tool and draw a line.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/draw_line_tool_screenshot.png)\\n  \\n* After drawing a line, click on the menu Plugins > Measurements (Plot Profile)\\n* If you modify the line, you may want to click the \"Refresh\" button to redraw the profile.\\n\\n![img.png](https://github.com/haesleinhuepf/napari-plot-profile/raw/main/docs/redraw_screenshot.png)\\n\\nTo see how these steps can be done programmatically from python, check out the [demo notebook](https://github.com/haesleinhuepf/napari-plot-profile/blob/main/docs/demo.ipynb)\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-plot-profile` via [pip]:\\n\\n    pip install napari-plot-profile\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-plot-profile\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[file an issue]: https://github.com/haesleinhuepf/napari-plot-profile/issues\\n\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[image.sc]: https://image.sc\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\\n\\n',\n",
       " 'napari-properties-plotter': '# napari-properties-plotter\\n\\n[![License](https://img.shields.io/pypi/l/napari-properties-plotter.svg?color=green)](https://github.com/brisvag/napari-properties-plotter/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-properties-plotter.svg?color=green)](https://pypi.org/project/napari-properties-plotter)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-plotter.svg?color=green)](https://python.org)\\n[![tests](https://github.com/brisvag/napari-properties-plotter/workflows/tests/badge.svg)](https://github.com/brisvag/napari-properties-plotter/actions)\\n[![codecov](https://codecov.io/gh/brisvag/napari-properties-plotter/branch/master/graph/badge.svg)](https://codecov.io/gh/brisvag/napari-properties-plotter)\\n\\nA napari plugin that automatically generates interactive plots based on layer properties.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-properties-plotter` via [pip]:\\n\\n    pip install napari-properties-plotter\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-properties-plotter\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/brisvag/napari-properties-plotter/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-properties-viewer': '# napari-properties-viewer\\n\\n[![License](https://img.shields.io/pypi/l/napari-properties-viewer.svg?color=green)](https://github.com/napari/napari-properties-viewer/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-properties-viewer.svg?color=green)](https://pypi.org/project/napari-properties-viewer)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-properties-viewer.svg?color=green)](https://python.org)\\n[![tests](https://github.com/kevinyamauchi/napari-properties-viewer/workflows/tests/badge.svg)](https://github.com/kevinyamauchi/napari-properties-viewer/actions)\\n[![codecov](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer/branch/master/graph/badge.svg)](https://codecov.io/gh/kevinyamauchi/napari-properties-viewer)\\n\\nA viewer for napari layer properties\\n\\n![image](resources/properties_viewer.gif)\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-properties-viewer` via [pip]:\\n\\n    pip install napari-properties-viewer\\n    \\n## Using the properties viewer table\\n\\n1. Open a a napari viewer with a layer with properties (e.g., Points)\\n2. View the properties by opening the properties viewer plugin from Plugins menu -> Add dock widget -> napari-propertiews-viewer: properties table\\n3. The layer property values are now displayed in the table widget. You can edit the values by double clicking the cell of interest and entering a new value.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-properties-viewer\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/kevinyamauchi/napari-properties-viewer/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-pyclesperanto-assistant': \"# napari-pyclesperanto-assistant\\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fclesperanto.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/clesperanto)\\n[![website](https://img.shields.io/website?url=http%3A%2F%2Fclesperanto.net)](http://clesperanto.net)\\n[![License](https://img.shields.io/pypi/l/napari-pyclesperanto-assistant.svg?color=green)](https://github.com/clesperanto/napari-pyclesperanto-assistant/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-pyclesperanto-assistant.svg?color=green)](https://pypi.org/project/napari-pyclesperanto-assistant)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-pyclesperanto-assistant.svg?color=green)](https://python.org)\\n[![tests](https://github.com/clesperanto/napari_pyclesperanto_assistant/workflows/tests/badge.svg)](https://github.com/clesperanto/napari_pyclesperanto_assistant/actions)\\n[![codecov](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant/branch/master/graph/badge.svg)](https://codecov.io/gh/clesperanto/napari_pyclesperanto_assistant)\\n\\nThe py-clEsperanto-assistant is a yet experimental [napari](https://github.com/napari/napari) plugin for building GPU-accelerated image processing workflows. \\nIt is part of the [clEsperanto](http://clesperanto.net) project and thus, aims at removing programming language related barriers between image processing ecosystems in the life sciences. \\nIt uses [pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) and with that [pyopencl](https://documen.tician.de/pyopencl/) as backend for processing images.\\nThis plugin was generated with [Cookiecutter](https://github.com/audreyr/cookiecutter) using with napari's [cookiecutter-napari-plugin](https://github.com/napari/cookiecutter-napari-plugin) template.\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/virtual_4d_support1.gif)\\n\\n## Installation\\n\\nIt is recommended to install the assistant via conda:\\n```shell\\nconda create --name bio11 python==3.8.5 \\nconda activate bio11 \\nconda install -c conda-forge pyopencl==2021.2.1\\npip install napari-pyclesperanto-assistant\\npip install napari[all]\\n```\\n\\nAlternatively, you can install the assistant using napari's plugin installer in the menu `Plugins > Install/uninstall Packages`.\\nWindows users should paste this URL\\n\\n```\\nhttps://github.com/clEsperanto/napari_pyclesperanto_assistant/blob/master/installation_help/pyopencl-2020.3.1+cl12-cp38-cp38-win_amd64.whl?raw=true\\n```\\n\\nin this field and click on `Install` before proceeding:\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot_installer2.png)\\n\\nAfterwards, click install clEsperanto like by clicking on `Install` here:\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot_installer.png)\\n\\nYou can then start napari, e.g. from command line, and find the assistant in the `Plugins` menu.\\n```shell\\nnapari\\n```\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot.png)\\n\\n## Features\\n[pyclesperanto](https://github.com/clEsperanto/pyclesperanto_prototype) offers various possibilities for processing images. It comes from developers who work in life sciences and thus, it may be focused towards processing two- and three-dimensional microscopy image data showing cells and tissues. A selection of pyclesperanto's functionality is available via the assistant user interface. Typical workflows which can be built with this assistant include\\n* image filtering\\n  * denoising / noise reduction (mean, median, Gaussian blur)\\n  * background subtraction for uneven illumination or out-of-focus light (bottom-hat, top-hat, subtract Gaussian background)\\n  * grey value morphology (local minimum, maximum. variance)\\n  * gamma correction\\n  * Laplace operator\\n  * Sobel operator\\n* combining images\\n  * masking\\n  * image math (adding, subtracting, multiplying, dividing images) \\n  * absolute / squared difference\\n* image transformations\\n  * translation\\n  * rotation\\n  * scale\\n  * reduce stack  \\n  * sub-stacks\\n* image projections\\n  * minimum / mean / maximum / sum / standard deviation projections\\n* image segmentation\\n  * binarization (thresholding, local maxima detection)\\n  * labeling\\n  * regionalization\\n  * instance segmentation\\n  * semantic segmentation\\n  * detect label edges\\n  * label spots\\n  * connected component labeling\\n  * Voronoi-Otsu-labeling\\n* post-processing of binary images\\n  * dilation\\n  * erosion\\n  * binary opening\\n  * binary closing \\n  * binary and / or / xor\\n* post-processing of label images\\n  * dilation (expansion) of labels\\n  * extend labels via Voronoi\\n  * exclude labels on edges\\n  * exclude labels within / out of size / value range\\n  * merge touching labels\\n* parametric maps\\n  * proximal / touching neighbor count\\n  * distance measurements to touching / proximal / n-nearest neighbors\\n  * pixel count map\\n  * mean / maximum / extension ratio map\\n* label measurements / post processing of parametric maps\\n  * minimum / mean / maximum / standard deviation intensity maps\\n  * minimum / mean / maximum / standard deviation of touching / n-nearest / neighbors\\n* neighbor meshes\\n  * touching neighbors\\n  * n-nearest neighbors\\n  * proximal neighbors\\n  * distance meshes\\n* measurements based on label images\\n  * bounding box 2D / 3D\\n  * minimum / mean / maximum / sum / standard deviation intensity\\n  * center of mass\\n  * centroid\\n  * mean / maximum distance to centroid (and extension ratio shape descriptor)\\n  * mean / maximum distance to center of mass (and extension ratio shape descriptor)\\n* code export\\n  * python / Fiji-compatible jython\\n  * python jupyter notebooks\\n* pyclesperanto scripting\\n  * cell segmentation\\n  * cell counting\\n  * cell differentiation\\n  * tissue classification\\n\\n## Usage\\n\\n### Start up the assistant\\nStart up napari, e.g. from the command line:\\n```\\nnapari\\n```\\n\\nLoad example data, e.g. from the menu `File > Open Samples > clEsperanto > CalibZAPWfixed` and \\nstart the assistant from the menu `Plugins > clEsperanto > Assistant`. Select a GPU in case you are asked to.\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1.png)\\n\\nIn case of two dimensional timelapse data, an initial conversion step might be necessary depending on your data source. \\nClick the menu `Plugins > clEsperanto > Convert to 2d timelapse`. In the dialog, select the dataset and click ok. \\nYou can delete the original dataset afterwards:\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot1a.png)\\n\\n### Set up a workflow\\n\\nChoose categories of operations in the top right panel, for example start with denoising using a Gaussian Blur with sigma 1 in x and y.\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2.png)\\n\\nContinue with background removal using the top-hat filter with radius 5 in x and y.\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2a.png)\\n\\nFor labeling the objects, use [Voronoi-Otsu-Labeling](https://nbviewer.jupyter.org/github/clEsperanto/pyclesperanto_prototype/blob/master/demo/segmentation/voronoi_otsu_labeling.ipynb) with both sigma parameters set to 2.\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2b.png)\\n\\nThe labeled objects can be extended using a Voronoi diagram to derive a estimations of cell boundaries.\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2c.png)\\n\\nYou can then configure napari to show the label boundaries on top of the original image:\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot2d.png)\\n\\nWhen your workflow is set up, click the play button below your dataset:\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/timelapse_2d.gif)\\n\\n### Code generation\\nYou can also export your workflow as Python/Jython code or as notebook.\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/screenshot3.png)\\n\\nAfter exporting your workflow as Jupyter notebook, you can start the notebook from the command line using\\n```\\njupyter notebook my_notebook.ipynb\\n```\\n\\nIn some cases you need to replace the command `cle.imread('None`)` with a command loading your image data. \\nAfter that, you can execute the notebook.\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/notebook.png)\\n\\nYou can also export code to the clipboard or as python code to disc. \\nThis python code can also be executed in [Fiji](https://fiji.sc)`s Jython, in case the [CLIJx-assistant is installed](https://clij.github.io/assistant/installation).\\n\\n![](https://github.com/clEsperanto/napari_pyclesperanto_assistant/raw/master/docs/images/fiji_execution.png)\\n\\nAlso note: The generated python/jython code is not capable of processing timelapse data,\\nyou need to program a for-loop processing timepoints individually yourself. \\n\\nWork in progress, contributions welcome.\\n\\n## For developers\\n\\nGetting the recent code from github and locally installing it\\n```\\ngit clone https://github.com/clesperanto/napari_pyclesperanto_assistant.git\\n\\npip install -e ./napari_pyclesperanto_assistant\\n```\\n\\nOptional: Also install pyclesperantos recent source code from github:\\n```\\ngit clone https://github.com/clEsperanto/pyclesperanto_prototype.git\\n\\npip install -e ./pyclesperanto_prototype\\n```\\n\\n## Feedback welcome!\\nclEsperanto is developed in the open because we believe in the open source community. See our [community guidelines](https://clij.github.io/clij2-docs/community_guidelines). Feel free to drop feedback as [github issue](https://github.com/clEsperanto/pyclesperanto_prototype/issues) or via [image.sc](https://image.sc)\\n\\n[Imprint](https://clesperanto.github.io/imprint)\\n\\n\\n\",\n",
       " 'napari-skimage-regionprops': '# napari-skimage-regionprops\\n\\n\\n\\n[![License](https://img.shields.io/pypi/l/napari-skimage-regionprops.svg?color=green)](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/LICENSE)\\n\\n[![PyPI](https://img.shields.io/pypi/v/napari-skimage-regionprops.svg?color=green)](https://pypi.org/project/napari-skimage-regionprops)\\n\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-skimage-regionprops.svg?color=green)](https://python.org)\\n\\n[![tests](https://github.com/haesleinhuepf/napari-skimage-regionprops/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-skimage-regionprops/actions)\\n\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-skimage-regionprops)\\n\\n\\n\\nA [napari] plugin for measuring properties of labeled objects based on [scikit-image]\\n\\n\\n\\n![](https://github.com/haesleinhuepf/napari-skimage-regionprops/raw/master/images/screenshot.png)\\n\\n\\n\\n## Features\\n\\nThe user can select categories of features for feature extraction in the user interface. These categories contain measurements from the scikit-image [regionprops list of measurements](https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprops) library:\\n\\n* size:\\n\\n  * area\\n\\n  * bbox_area\\n\\n  * convex_area\\n\\n  * equivalent_diameter\\n\\n* intensity:\\n\\n  * max_intensity \\n\\n  * mean_intensity\\n\\n  * min_intensity\\n\\n  * standard_deviation_intensity (`extra_properties` implementation using numpy)\\n\\n* perimeter:\\n\\n  * perimeter\\n\\n  * perimeter_crofton\\n\\n* shape\\n\\n  * major_axis_length\\n\\n  * minor_axis_length\\n\\n  * orientation\\n\\n  * solidity\\n\\n  * eccentricity\\n\\n  * extent\\n\\n  * feret_diameter_max\\n\\n  * local_centroid\\n\\n* position:\\n\\n  * centroid\\n\\n  * bbox\\n\\n  * weighted_centroid\\n\\n* moments:\\n\\n  * moments\\n\\n  * moments_central\\n\\n  * moments_hu\\n\\n  * moments_normalized\\n\\n\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n\\n\\n## Installation\\n\\n\\n\\nYou can install `napari-skimage-regionprops` via [pip]:\\n\\n\\n\\n    pip install napari-skimage-regionprops\\n\\n\\n\\nOr if you plan to develop it:\\n\\n\\n\\n    git clone https://github.com/haesleinhuepf/napari-skimage-regionprops\\n\\n    cd napari-skimage-regionprops\\n\\n    pip install -e .\\n\\n\\n\\nIf there is an error message suggesting that git is not installed, run `conda install git`.\\n\\n\\n\\n## Contributing\\n\\n\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\n\\nthe coverage at least stays the same before you submit a pull request.\\n\\n\\n\\n## License\\n\\n\\n\\nDistributed under the terms of the [BSD-3] license,\\n\\n\"napari-skimage-regionprops\" is free and open source software\\n\\n\\n\\n## Issues\\n\\n\\n\\nIf you encounter any problems, please create a thread on [image.sc] along with a detailed description and tag [@haesleinhuepf].\\n\\n\\n\\n[napari]: https://github.com/napari/napari\\n\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n\\n[@napari]: https://github.com/napari\\n\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n\\n[image.sc]: https://image.sc\\n\\n[napari]: https://github.com/napari/napari\\n\\n[tox]: https://tox.readthedocs.io/en/latest/\\n\\n[pip]: https://pypi.org/project/pip/\\n\\n[PyPI]: https://pypi.org/\\n\\n[scikit-image]: https://scikit-image.org/\\n\\n[@haesleinhuepf]: https://twitter.com/haesleinhuepf\\n\\n\\n\\n',\n",
       " 'napari-stracking': '# Description\\n\\nThe STracking suite provides a set of plugins for particles tracking in 2D+t and 3D+t images. \\nA classical particles tracking pipeline is made of 5 sequential steps:\\n\\n* Particles detection frame by frame\\n\\n![img](https://raw.githubusercontent.com/sylvainprigent/napari-stracking/main/docs/images/sdogdetector_res.png)\\n\\n\\n* Particles properties calculation (optional)\\n\\n![img](https://raw.githubusercontent.com/sylvainprigent/napari-stracking/main/docs/images/sparticlesproperties_res.png?raw=true)\\n\\n\\n* Particles linking\\n\\n![img](https://raw.githubusercontent.com/sylvainprigent/napari-stracking/main/docs/images/slinkershortestpath_res.png?raw=true)\\n\\n\\n* Tracks features extraction (optional)\\n\\n![img](https://raw.githubusercontent.com/sylvainprigent/napari-stracking/main/docs/images/stracksfeatures_res.png?raw=true)\\n\\n\\n* Tracks filtering (optional)\\n\\n![img](https://raw.githubusercontent.com/sylvainprigent/napari-stracking/main/docs/images/sfiltertracks_res.png?raw=true)\\n\\n\\n## Installation\\n\\nYou can install `napari-stracking` via [pip]:\\n\\n    pip install napari-stracking\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\"napari-tracks-reader\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sylvainprigent/napari-strcking/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/',\n",
       " 'napari-svg': '# napari-svg\\n\\n[![License](https://img.shields.io/pypi/l/napari-svg.svg?color=green)](https://github.com/napari/napari-svg/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-svg.svg?color=green)](https://pypi.org/project/napari-svg)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-svg.svg?color=green)](https://python.org)\\n[![tests](https://github.com/napari/napari-svg/workflows/tests/badge.svg)](https://github.com/napari/napari-svg/actions)\\n[![codecov](https://codecov.io/gh/napari/napari-svg/branch/master/graph/badge.svg)](https://codecov.io/gh/napari/napari-svg)\\n\\nA plugin for reading and writing svg files with napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `napari-svg` via [pip]:\\n\\n    pip install napari-svg\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-svg\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/napari/napari-svg/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-tracks-reader': '# Description\\n\\nThis plugin allows to open particle tracking results from multiple formats into the Napari \\nTracks Layer.\\n\\n# Supported formats\\n\\nThe formats currently supported by this plugin are:\\n\\n## CSV\\n\\nThe most basic format to store particle tracking tracks is a CSV file containing the tracks table.\\nIn this format each line is a particle and each column a property of the particle. The table\\nheaders must be `TrackID`, `t`, `x`, `y`, `z`. Note that the header order does not matter:\\n\\n| TrackID       | t | x | y | z |\\n| :------------ | :----------: | :----------: | :----------: | -----------: |\\n| 0 | 16   | 41.5828343348868  | 47.505930020081664| 0 |\\n| 0 | 17   | 41.48425270538317 | 51.6023835597057 | 0 |\\n\\nThe raw CSV file is a classical comma-separated values format: \\n\\n```csv\\nTrackID,t,x,y,z\\n0,16, 41.5828343348868, 47.505930020081664, 0\\n0,17, 41.48425270538317, 51.6023835597057, 0\\n...\\n```\\n\\n[!NOTE]\\nThis CSV format does **not** support split and merge events\\n\\n## TrackMate\\n\\nThe TrackMate format is the XML model file générated by the [TrackMate](https://imagej.net/plugins/trackmate/) Fiji \\nplugin. \\nThe XML file from TrackMate should not be manually modified and and contains a `Model` element:\\n\\n```xml\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<TrackMate version=\"6.0.2\">\\n...\\n<Model spatialunits=\"pixel\" timeunits=\"sec\">\\n    <FeatureDeclarations>\\n      <SpotFeatures>\\n        <Feature feature=\"QUALITY\" name=\"Quality\" shortname=\"Quality\" dimension=\"QUALITY\" isint=\"false\" />\\n        <Feature feature=\"POSITION_X\" name=\"X\" shortname=\"X\" dimension=\"POSITION\" isint=\"false\" />\\n...\\n```\\n\\nAll the particles features from the TrackMate model file are loaded in the napari tracks properties. \\n\\n[!NOTE]\\nThis format supports split and merge events\\n\\n## Icy\\n\\nThe Icy format is a XML file generated by the [Icy](http://icy.bioimageanalysis.org/plugin/spot-tracking/) software.\\nThe XML file from ICY should not be manually modified and starts with the `root` element:\\n\\n```xml\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<root>\\n  <trackfile version=\"1\" />\\n  <trackgroup>\\n    <track id=\"265713726\">\\n      <detection t=\"16\" x=\"41.5828343348868\" y=\"47.505930020081664\" z=\"0\" classname=\"plugins.nchenouard.particleTracking.sequenceGenerator.ProfileSpotTrack\" type=\"1\" />\\n      <detection t=\"17\" x=\"41.48425270538317\" y=\"51.6023835597057\" z=\"0\" classname=\"plugins.nchenouard.particleTracking.sequenceGenerator.ProfileSpotTrack\" type=\"1\" />\\n      ...\\n```\\n\\n[!TIP]\\nThis format supports split and merge events\\n\\n## ISBI\\n\\nThe ISBI format is a XML format used for the ISBI tracking challenge. This format must contain \\na `root` element and a list of particles in a `TrackContestISBI2012` element:\\n\\n```xml\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<root>\\n  <TrackContestISBI2012 snr=\"?\" density=\"?\" scenario=\"FakeTracks.tif\" generationDateTime=\"Wed May 12 14:07:09 CEST 2021\">\\n    <particle>\\n      <detection t=\"0\" x=\"64.00558680057657\" y=\"3.9587411612103076\" z=\"0.0\" />\\n      <detection t=\"1\" x=\"63.98171495578894\" y=\"6.04150382894106\" z=\"0.0\" />\\n      <detection t=\"2\" x=\"63.95406806092088\" y=\"10.06085170348766\" z=\"0.0\" />\\n``` \\n\\n[!NOTE]\\nThis format does **not** support split and merge events\\n\\n\\n# Quickstart\\n\\nYou can open local tracks using `napari` at the terminal and the path to your file:\\n\\n```\\n$ napari /path/to/your/tracks.xml\\n```\\n\\nOR in python:\\n\\n```python\\nimport napari\\n\\nviewer = napari.Viewer()\\nviewer.open(\\'/path/to/your/tracks.xml\\')\\nnapari.run()\\n```\\n\\n# Getting Help\\n\\nIf you discover a bug with the plugin, or would like to request a new feature, please\\nraise an issue on our repository at https://github.com/sylvainprigent/napari-tracks-reader.\\n',\n",
       " 'napari-webcam': '# napari-webcam\\n\\n[![License](https://img.shields.io/pypi/l/napari-webcam.svg?color=green)](https://github.com/haesleinhuepf/napari-webcam/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-webcam.svg?color=green)](https://pypi.org/project/napari-webcam)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-webcam.svg?color=green)](https://python.org)\\n[![tests](https://github.com/haesleinhuepf/napari-webcam/workflows/tests/badge.svg)](https://github.com/haesleinhuepf/napari-webcam/actions)\\n[![codecov](https://codecov.io/gh/haesleinhuepf/napari-webcam/branch/master/graph/badge.svg)](https://codecov.io/gh/haesleinhuepf/napari-webcam)\\n\\nUse your webcam from within napari!\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n## Installation\\n\\nYou can install `napari-webcam` via [pip]:\\n\\n    pip install napari-webcam\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"napari-webcam\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please open a thread on [image.sc] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[image.sc]: https://image.sc\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'napari-yapic-prediction': '# napari-yapic-prediction\\n\\n[![License](https://img.shields.io/pypi/l/napari-yapic-prediction.svg?color=green)](https://github.com/yapic/napari-yapic-prediction/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/napari-yapic-prediction.svg?color=green)](https://pypi.org/project/napari-yapic-prediction)\\n[![Python Version](https://img.shields.io/pypi/pyversions/napari-yapic-prediction.svg?color=green)](https://python.org)\\n[![tests](https://github.com/yapic/napari-yapic-prediction/workflows/tests/badge.svg)](https://github.com/yapic/napari-yapic-prediction/actions)\\n[![codecov](https://codecov.io/gh/yapic/napari-yapic-prediction/branch/master/graph/badge.svg?token=amah2YwOpx)](https://codecov.io/gh/yapic/napari-yapic-prediction)\\n\\nnapari widget plugin to perform YAPiC model segmentation prediction in the napari window.\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Description\\n\\nThis napari plugin provides a widget to upload a [YAPiC] trained model and perform segmentation over all the present images in the napari window. The segmentation results are uploaded as napari layers into the viewer automatically with the name structure of *imgename_prediction*.\\n\\n## Installation\\n\\n1. Please install either GPU or CPU version of tensorflow before installing the plugin depending on your system.\\nOne of the plugin dependency is `yapic` that currently has sensitivity to tensorflow versions.\\nThis behaviour will be removed in future.\\n\\n2. You can install `napari-yapic-prediction` via [pip]:\\n\\n    pip install napari-yapic-prediction\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [GNU GPL v3.0] license,\\n\"napari-yapic-prediction\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/yapic/napari-yapic-prediction/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n[YAPiC]: https://yapic.github.io/yapic/\\n\\n\\n',\n",
       " 'napari_video': \"# napari-video\\nNapari plugin for working with videos.\\n\\nRelies on [pyvideoreader](https://pypi.org/project/pyvideoreader/) as a backend which itself uses [opencv](https://opencv.org) for reading videos.\\n\\n## Installation\\n```shell\\npip install napari[all] napari_video\\n```\\n\\n## Usage\\nFrom a terminal:\\n```shell\\nnapari video.avi\\n```\\n\\nOr from within python:\\n```shell\\nimport napari\\nfrom napari_video.napari_video import VideoReaderNP\\n\\npath='video.mp4'\\nvr = VideoReaderNP(path)\\nwith napari.gui_qt():\\n    viewer = napari.view_image(vr, name=path)\\n```\\n\\n## Internals\\n`napari_video.napari_video.VideoReaderNP` exposes a video with a numpy-like interface, using opencv as a backend.\\n\\nFor instance, open a video:\\n```python\\nvr = VideoReaderNP('video.avi')\\nprint(vr)\\n```\\n```\\nvideo.avi with 60932 frames of size (920, 912, 3) at 100.00 fps\\n```\\nThen\\n\\n- `vr[100]` will return the 100th frame as a numpy array with shape `(902, 912, 3)`.\\n- `vr[100:200:10]` will return 10 frames evenly spaced between frame number 100 and 200 (shape `(10, 902, 912, 3)`).\\n- Note that by default, single-frame and slice indexing return 3D and 4D arrays, respectively. To consistently return 4D arrays, open the video with `remove_leading_singleton=False`. `vr[100]` will then return a `(1, 902, 912, 3)` array.\\n- We can also request specific ROIs and channels. For instance, `vr[100:200:10,100:400,800:850,1]` will return an array with shape `(10, 300, 50, 1)`.\\n\\n\",\n",
       " 'nd2-dask': '# nd2-dask\\n\\n[![License](https://img.shields.io/pypi/l/nd2-dask.svg?color=green)](https://github.com/napari/nd2-dask/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/nd2-dask.svg?color=green)](https://pypi.org/project/nd2-dask)\\n[![Python Version](https://img.shields.io/pypi/pyversions/nd2-dask.svg?color=green)](https://python.org)\\n[![tests](https://github.com/DragaDoncila/nd2-dask/workflows/tests/badge.svg)](https://github.com/DragaDoncila/nd2-dask/actions)\\n[![codecov](https://codecov.io/gh/DragaDoncila/nd2-dask/branch/master/graph/badge.svg)](https://codecov.io/gh/DragaDoncila/nd2-dask)\\n\\nPlugin to load nd2 data into napari\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `nd2-dask` via [pip]:\\n\\n    pip install nd2-dask\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"nd2-dask\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/DragaDoncila/nd2-dask/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n',\n",
       " 'platelet-unet-watershed': '# platelet-unet-watershed\\n\\n[![License](https://img.shields.io/pypi/l/platelet-unet-watershed.svg?color=green)](https://github.com/jni/platelet-unet-watershed/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/platelet-unet-watershed.svg?color=green)](https://pypi.org/project/platelet-unet-watershed)\\n[![Python Version](https://img.shields.io/pypi/pyversions/platelet-unet-watershed.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jni/platelet-unet-watershed/workflows/tests/badge.svg)](https://github.com/jni/platelet-unet-watershed/actions)\\n[![codecov](https://codecov.io/gh/jni/platelet-unet-watershed/branch/master/graph/badge.svg)](https://codecov.io/gh/jni/platelet-unet-watershed)\\n\\nSegment platelets with pretrained unet and affinity watershed\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `platelet-unet-watershed` via [pip]:\\n\\n    pip install platelet-unet-watershed\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"platelet-unet-watershed\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/jni/platelet-unet-watershed/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/',\n",
       " 'stardist-napari': \"# StarDist Napari Plugin\\n\\n[![PyPI version](https://img.shields.io/pypi/v/stardist-napari.svg)](https://pypi.org/project/stardist-napari)\\n[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fstardist.json&query=%24.topic_list.tags.0.topic_count&colorB=brightgreen&suffix=%20topics&logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/stardist)\\n\\nThis project provides the [napari](https://napari.org/) plugin for [StarDist](https://github.com/stardist/stardist), a deep learning based 2D and 3D object detection method with star-convex shapes. StarDist has originally been developed (see [papers](https://github.com/stardist/stardist#stardist---object-detection-with-star-convex-shapes)) for the segmentation of densely packed cell nuclei in challenging images with low signal-to-noise ratios. The plugin allows to apply pretrained and custom trained models from within napari.\\n\\n![Screenshot](https://github.com/stardist/stardist-napari/raw/main/images/stardist_napari_screenshot_small.png)\\n\\n## Installation & Usage\\n\\nInstall the plugin with `pip install stardist-napari` or from within napari via `Plugins > Install/Uninstall Package(s)…`. If you want GPU-accelerated prediction, please read the more detailed [installation instructions](https://github.com/stardist/stardist#installation) for StarDist.\\n\\nYou can activate the plugin in napari via `Plugins > StarDist: StarDist`. Example images for testing are provided via `File > Open Sample > StarDist`.\\n\\nFor a more detailed demonstration of the plugin, please [watch this short video](https://www.youtube.com/watch?v=Km1_TnUQ4FM&list=PLilvrWT8aLuZCaOkjucLjvDu2YRtCS-JT&index=5).\\n\\nThere's no dedicated documentation yet, but the most important parameters are identical to those of our [StarDist ImageJ/Fiji plugin](https://github.com/stardist/stardist-imagej), which are documented [here](https://imagej.net/plugins/stardist#usage).\\n\\nIf you use this plugin for your research, please [cite us](https://github.com/stardist/stardist#how-to-cite).\\n\\n## Troubleshooting & Support\\n\\n- The [image.sc forum](https://forum.image.sc/tag/stardist) is the best place to start getting help and support. Make sure to use the tag `stardist`, since we are monitoring all questions with this tag.\\n- For general questions about StarDist, it's worth taking a look at the [frequently asked questions (FAQ)]( https://stardist.net/docs/faq.html).\\n- If you have technical questions or found a bug, feel free to [open an issue](https://github.com/stardist/stardist-napari/issues).\\n\\n\\n\",\n",
       " 'waver': '# waver\\n\\n[![License](https://img.shields.io/pypi/l/waver.svg?color=green)](https://github.com/sofroniewn/waver/raw/master/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/waver.svg?color=green)](https://pypi.org/project/waver)\\n[![Python Version](https://img.shields.io/pypi/pyversions/waver.svg?color=green)](https://python.org)\\n[![tests](https://github.com/sofroniewn/waver/workflows/tests/badge.svg)](https://github.com/sofroniewn/waver/actions)\\n[![codecov](https://codecov.io/gh/sofroniewn/waver/branch/main/graph/badge.svg?token=QBP7K6YUT7)](https://codecov.io/gh/sofroniewn/waver)\\n\\nRun simulations of the [wave equation](https://en.wikipedia.org/wiki/Wave_equation) in nD on grids of variable speed in Python. This library owes a lot of its design and approach to the [fdtd](https://github.com/flaport/fdtd) library, a Python 3D electromagnetic FDTD simulator.\\n\\nThis package allows for a fair amount of customization over your wave simulation. You can\\n - specify the size and spacing of the grid\\n - specify the time step for the simulation, which will be checked to ensure stability of the simulation\\n - specify the duration of the simulation\\n - setting a variable speed array (one value per grid point) to allow for \"objects\" in your environment\\n - set the source of the wave, which can be a point, line, or any (n-1)D subarray\\n - record the wave with a detector, which can be the full grid, the full boundary, or a particular boundary\\n - use convenience methods to run many simulations with different sources on the same grid and detector combination\\n\\nYou can use [napari](https://napari.org/), a multi-dimensional image viewer for Python, to allow for easy visualization of the detected wave. Some functionality is also available as a napari plugin to allow for running simulations from a graphical user interface.\\n\\nResults can look like\\n\\nhttps://user-images.githubusercontent.com/6531703/128283012-a784ec06-4df9-4ddf-bf4f-e21b927fe4a3.mov\\n\\n----------------------------------\\n\\n## Installation\\n\\nYou can install `waver` via [pip]:\\n\\n    pip install waver\\n\\n## Usage\\n\\n### Convenience Methods\\n\\nThe most convenient way to use waver is to use one of two convenience methods that will create and run a simulation\\nfor you and return the results.\\n\\nThe first method `run_single_source` allows you to run a single simulation with a single source on one grid and \\nrecord the results using a detector. For example\\n\\n```python\\nfrom waver.simulation import run_single_source\\n\\nsingle_sim_params = {\\n    \\'size\\': (12.8e-3, 12.8e-3),\\n    \\'spacing\\': 100e-6,\\n    \\'duration\\': 80e-6,\\n    \\'min_speed\\': 343,\\n    \\'max_speed\\': 686,\\n    \\'speed\\': 686,\\n    \\'time_step\\': 50e-9,\\n    \\'temporal_downsample\\': 2,\\n    \\'location\\': (6.4e-3, 6.4e-3),\\n    \\'period\\': 5e-6,\\n    \\'ncycles\\':1,\\n}\\n\\ndetected_wave, speed_grid = run_single_source(**single_sim_params)\\n```\\n\\nThe second method `run_multiple_sources` allows you to run multiple simulations with multiple sources on the same\\ngrid and with the same detector and return the results. For example\\n\\n```python\\nfrom waver.simulation import run_multiple_sources\\n\\nmulti_sim_params = {\\n    \\'size\\': (12.8e-3, 12.8e-3),\\n    \\'spacing\\': 100e-6,\\n    \\'duration\\': 80e-6,\\n    \\'min_speed\\': 343,\\n    \\'max_speed\\': 686,\\n    \\'speed\\': 686,\\n    \\'time_step\\': 50e-9,\\n    \\'temporal_downsample\\': 2,\\n    \\'sources\\': [{\\n        \\'location\\': (6.4e-3, 6.4e-3),\\n        \\'period\\': 5e-6,\\n        \\'ncycles\\':1,\\n    }]\\n}\\n\\ndetected_wave, speed_grid = run_multiple_sources(**multi_sim_params)\\n```\\n\\nThe main difference between these two methods is that `run_multiple_sources` takes a `sources` parameter which takes a list \\nof dictionaries with keys corresponding to source related keyword arguments found in `run_single_source`.\\n\\n### Visualization\\n\\nIf you want to quickly visualize the results of `run_multiple_sources`, you can use the `run_and_visualize` command which will \\nrun the simulation and then launch napari with the results, as seen in [examples/2D/point_source.py](./examples/2D/point_source.py)\\n\\n```python\\nfrom waver.datasets import run_and_visualize\\n\\nrun_and_visualize(**multi_sim_params)\\n```\\n\\n### Datasets\\n\\nIf you want to run simulations with on many different speed grids you can use the `generate_simulation_dataset` method as a convenience. The results will be saved to a [zarr](https://zarr.readthedocs.io/en/stable/) file of your chosing. You can then use the `load_simulation_dataset` to load the dataset.\\n\\n```python\\nfrom waver.datasets import generate_simulation_dataset\\n\\n# Define root path for simulation\\npath = \\'./simulation_dataset.zarr\\'\\nruns = 5\\n\\n# Define a simulation, 12.8mm, 100um spacing\\ndataset_sim_params = {\\n    \\'size\\': (12.8e-3, 12.8e-3),\\n    \\'spacing\\': 100e-6,\\n    \\'duration\\': 80e-6,\\n    \\'min_speed\\': 343,\\n    \\'max_speed\\': 686,\\n    \\'speed\\': \\'mixed_random_ifft\\',\\n    \\'time_step\\': 50e-9,\\n    \\'sources\\': [{\\n        \\'location\\': (None, 0),\\n        \\'period\\': 5e-6,\\n        \\'ncycles\\':1,\\n    }],\\n    \\'temporal_downsample\\': 2,\\n    \\'boundary\\': 1,\\n    \\'edge\\': 1,\\n}\\n\\n# Run and save simulation\\ngenerate_simulation_dataset(path, runs, **dataset_sim_params)\\n```\\n\\nThe `generate_simulation_dataset` allows the `speed` to be a string that will specify a particular method of randomly generating speed values for the simulation grid.\\n\\n### The Simulation Object\\n\\nIf you\\'d like to understand in a little bit more detail how a simulation is defined then you might want to use the unerlying simulation object `Simulation` and manually set key objects like the `Source` and `Detector`. A full example of this is as follows\\n\\n```python\\n# Create a simulation\\nsim = Simulation(size=size, spacing=spacing, max_speed=max_speed, time_step=time_step)\\n\\n# Set speed array\\nsim.set_speed(speed=speed, min_speed=min_speed, max_speed=max_speed)\\n\\n# Add source\\nsim.add_source(location=location, period=period, ncycles=ncycles, phase=phase)\\n\\n# Add detector grid\\nsim.add_detector(spatial_downsample=spatial_downsample,\\n                    boundary=boundary, edge=edge)\\n\\n# Run simulation\\nsim.run(duration=duration, temporal_downsample=temporal_downsample, progress=progress, leave=leave)\\n\\n# Print simulation wave and speed data\\nprint(\\'wave: \\', sim.detected_wave)\\nprint(\\'speed: \\', sim.grid_speed)\\n```\\n\\nNote these steps are done inside the `run_single_source` method for you as a convenience.\\n\\n## Known Limitations\\n\\nA [perfectly matched layer](https://en.wikipedia.org/wiki/Perfectly_matched_layer) boundary has recently been added, but might not perform well under all conditions. Additional contributions would be welcome here.\\n\\nRight now the simulations are quite slow. I\\'d like to add a [JAX](https://github.com/google/jax) backend, but \\nhavn\\'t done so yet. Contributions would be welcome.\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"waver\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/sofroniewn/waver/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/\\n\\n\\n',\n",
       " 'zarpaint': '# zarpaint\\n\\n[![License](https://img.shields.io/pypi/l/zarpaint.svg?color=green)](https://github.com/jni/zarpaint/raw/main/LICENSE)\\n[![PyPI](https://img.shields.io/pypi/v/zarpaint.svg?color=green)](https://pypi.org/project/zarpaint)\\n[![Python Version](https://img.shields.io/pypi/pyversions/zarpaint.svg?color=green)](https://python.org)\\n[![tests](https://github.com/jni/zarpaint/workflows/tests/badge.svg)](https://github.com/jni/zarpaint/actions)\\n[![codecov](https://codecov.io/gh/jni/zarpaint/branch/main/graph/badge.svg)](https://codecov.io/gh/jni/zarpaint)\\n\\nPaint segmentations directly to on-disk/remote zarr arrays\\n\\n----------------------------------\\n\\nThis [napari] plugin was generated with [Cookiecutter] using with [@napari]\\'s [cookiecutter-napari-plugin] template.\\n\\n<!--\\nDon\\'t miss the full getting started guide to set up your new package:\\nhttps://github.com/napari/cookiecutter-napari-plugin#getting-started\\n\\nand review the napari docs for plugin developers:\\nhttps://napari.org/docs/plugins/index.html\\n-->\\n\\n## Installation\\n\\nYou can install `zarpaint` via [pip]:\\n\\n    pip install zarpaint\\n\\n## Contributing\\n\\nContributions are very welcome. Tests can be run with [tox], please ensure\\nthe coverage at least stays the same before you submit a pull request.\\n\\n## License\\n\\nDistributed under the terms of the [BSD-3] license,\\n\"zarpaint\" is free and open source software\\n\\n## Issues\\n\\nIf you encounter any problems, please [file an issue] along with a detailed description.\\n\\n[napari]: https://github.com/napari/napari\\n[Cookiecutter]: https://github.com/audreyr/cookiecutter\\n[@napari]: https://github.com/napari\\n[MIT]: http://opensource.org/licenses/MIT\\n[BSD-3]: http://opensource.org/licenses/BSD-3-Clause\\n[GNU GPL v3.0]: http://www.gnu.org/licenses/gpl-3.0.txt\\n[GNU LGPL v3.0]: http://www.gnu.org/licenses/lgpl-3.0.txt\\n[Apache Software License 2.0]: http://www.apache.org/licenses/LICENSE-2.0\\n[Mozilla Public License 2.0]: https://www.mozilla.org/media/MPL/2.0/index.txt\\n[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin\\n[file an issue]: https://github.com/jni/zarpaint/issues\\n[napari]: https://github.com/napari/napari\\n[tox]: https://tox.readthedocs.io/en/latest/\\n[pip]: https://pypi.org/project/pip/\\n[PyPI]: https://pypi.org/'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c36a992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PartSeg',\n",
       " 'brainreg-napari',\n",
       " 'brainreg-segment',\n",
       " 'cellpose-napari',\n",
       " 'misic-napari-plugin',\n",
       " 'napari-allencell-segmenter',\n",
       " 'napari-arboretum',\n",
       " 'napari-deepmeta',\n",
       " 'napari-hdf5-labels-io',\n",
       " 'napari-oclrfc',\n",
       " 'napari-pyclesperanto-assistant',\n",
       " 'napari-yapic-prediction',\n",
       " 'stardist-napari',\n",
       " 'zarpaint']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d for d in list(descriptions.keys()) if \"segmentation\" in descriptions[d]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86d81c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-21 17:14:10,228 [INFO] WRITING LOG OUTPUT TO C:\\Users\\rober\\.cellpose\\run.log\n",
      "2021-08-21 17:14:10,418 [INFO] Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-08-21 17:14:10,419 [INFO] NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Resource 'XMLSchema.xsd' is already loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-21 17:14:14,131 [INFO] Resource 'XMLSchema.xsd' is already loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<napari.plugins._plugin_manager.NapariPluginManager at 0x249d7e6e0d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import napari\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "\n",
    "from napari.plugins import plugin_manager\n",
    "\n",
    "plugin_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08052b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clEsperanto\n",
      "* Assistant\n",
      "* Remove noise\n",
      "* Remove background\n",
      "* Filter\n",
      "* Combine\n",
      "* Transform\n",
      "* Projection\n",
      "* Binarize\n",
      "* Label\n",
      "* Process labels\n",
      "* Measure labels\n",
      "* Measure labeled image\n",
      "* Mesh\n",
      "* Label neighbor filters\n",
      "Visualization(B/C)\n",
      "* Brightness Contrast\n",
      "ome-types\n",
      "* OME Metadata Viewer\n",
      "napari-yapic-prediction\n",
      "* My Widget\n",
      "omero\n",
      "* browser\n",
      "cellpose-napari\n",
      "* cellpose\n",
      "clEsperanto\n",
      "* statistics of labeled pixels\n",
      "* make labels editable\n",
      "* auto brightness contrast\n",
      "* auto brightness contrast all images\n",
      "* reset brightness contrast\n",
      "* split stack\n",
      "* set voxel size\n",
      "* set voxel size of all layers\n",
      "* convert image to labels\n",
      "* convert labels to image\n",
      "* convert to numpy\n",
      "* convert to 2d timelapse\n",
      "Segmentation (split/merge)\n",
      "* Manually merge labels\n",
      "* Manually split labels\n",
      "Segmentation (OCLRFC)\n",
      "* Train pixel classifier\n",
      "* Predict pixel classifier\n",
      "* Connected component labeling\n",
      "* Train label classifier\n",
      "* Predict label classifier\n",
      "Utilities (skimage regionprops)\n",
      "* duplicate current frame\n",
      "Measurements (skimage regionprops)\n",
      "* skimage regionprops\n"
     ]
    }
   ],
   "source": [
    "for hook_type, (plugin_name, widgets) in plugin_manager.iter_widgets():\n",
    "    print (plugin_name)\n",
    "    for widget in widgets:\n",
    "        print(\"*\", widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88e556b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function napari_manual_split_and_merge_labels._function.Manually_merge_labels(labels_layer: napari.layers.labels.labels.Labels, points_layer: napari.layers.points.points.Points, viewer: napari.viewer.Viewer)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_widget(plugin_name, widget_name):\n",
    "    from napari.plugins import plugin_manager\n",
    "    if plugin_name in plugin_manager._function_widgets:\n",
    "        functions = plugin_manager._function_widgets[plugin_name]\n",
    "        if widget_name in functions:\n",
    "            return functions[widget_name]\n",
    "    return plugin_manager.get_widget(plugin_name, widget_name)\n",
    "\n",
    "get_widget(\"Segmentation (split/merge)\", \"Manually merge labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc19370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
